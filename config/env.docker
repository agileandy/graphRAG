# Neo4j Configuration (for Docker)
NEO4J_URI=bolt://0.0.0.0:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=graphrag

# ChromaDB Configuration (for Docker)
CHROMA_PERSIST_DIRECTORY=/app/data/chromadb

# Port Configuration (for Docker)
# API Services
GRAPHRAG_PORT_API=5000                # Main GraphRAG API (Docker uses 5000 instead of 5001)

# MPC Services
GRAPHRAG_PORT_MPC=8765                # Message Passing Communication server

# MCP Services
GRAPHRAG_PORT_MCP=8767                # Model Context Protocol server
GRAPHRAG_PORT_BUG_MCP=5005            # Bug tracking MCP server

# Database Services
GRAPHRAG_PORT_NEO4J_BOLT=7687         # Neo4j Bolt protocol
GRAPHRAG_PORT_NEO4J_HTTP=7474         # Neo4j HTTP API
GRAPHRAG_PORT_NEO4J_HTTPS=7473        # Neo4j HTTPS API

# Monitoring Services
GRAPHRAG_PORT_PROMETHEUS=9090         # Prometheus metrics
GRAPHRAG_PORT_GRAFANA=3000            # Grafana dashboard

# Docker Port Mappings
# Note: These are the host ports that map to container ports
GRAPHRAG_PORT_DOCKER_NEO4J_BOLT=7688  # Docker mapping for Neo4j Bolt

# LLM Configuration (for Docker)
# Note: When running in Docker, you'll need to adjust these endpoints
# to point to services accessible from within the container
LLM_ENDPOINT=http://host.docker.internal:1234  # LM Studio on host machine
OLLAMA_ENDPOINT=http://host.docker.internal:11434  # Ollama on host machine
EMBEDDING_MODEL=snowflake-arctic-embed2:latest
RERANKER_MODEL=qllama/bge-reranker-large:latest
CONCEPT_MODEL=lmstudio-community/Phi-4-mini-reasoning-MLX-4bit
USE_LOCAL_LLM=true