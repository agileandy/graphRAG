# Specification: Enhance Graph Capabilities (Phase 3.2)

## Overview

This specification outlines the improvements needed for graph capabilities in the GraphRAG system, focusing on improving graph visualization and refining graph queries for better performance and functionality.

## 1. Improve Graph Visualization

### Problem Statement

The current graph visualization lacks filtering options, interactive exploration capabilities, and concept clustering for better organization.

### Requirements

1. **Enhanced Visualization Script**
   - Add filtering options by concept type, relationship strength, and document count
   - Implement color coding for different concept types and relationship strengths
   - Add ability to focus on specific concepts and their immediate connections
   - Support for exporting visualization data in multiple formats

2. **Interactive Graph Exploration**
   - Implement interactive graph navigation with zoom and pan capabilities
   - Add click-to-expand functionality for exploring related concepts
   - Implement search within the visualization to quickly find concepts
   - Add tooltips with detailed information on hover

3. **Concept Clustering**
   - Implement automatic clustering of related concepts
   - Add manual grouping capabilities for user-defined organization
   - Visualize concept clusters with distinct visual boundaries
   - Support for collapsing and expanding clusters

4. **Export Options**
   - Add export to common visualization formats (PNG, SVG, PDF)
   - Support for exporting graph data to formats compatible with other visualization tools
   - Implement export of subgraphs based on user selection
   - Add options for customizing export appearance

### Implementation Details

#### Enhanced Visualization Script

```python
def visualize_graph(filters=None, focus_concept=None, color_by='type'):
    """
    Generate an enhanced graph visualization with filtering and focus options.

    Args:
        filters (dict): Filtering criteria (e.g., concept_type, min_relationship_strength)
        focus_concept (str): Central concept to focus visualization around
        color_by (str): Property to use for color coding ('type', 'strength', 'count')

    Returns:
        networkx.Graph: The generated graph visualization
    """
    import networkx as nx
    import matplotlib.pyplot as plt
    from pyvis.network import Network

    # Connect to Neo4j
    driver = get_neo4j_driver()

    # Build query based on filters
    query = """
    MATCH (c:Concept)
    OPTIONAL MATCH (c)-[r:RELATED_TO]-(other:Concept)
    """

    # Add filter conditions
    if filters:
        conditions = []
        if 'concept_type' in filters:
            conditions.append(f"c.type = '{filters['concept_type']}'")
        if 'min_relationship_strength' in filters:
            conditions.append(f"r.strength >= {filters['min_relationship_strength']}")
        if 'min_document_count' in filters:
            conditions.append(f"c.document_count >= {filters['min_document_count']}")

        if conditions:
            query += "WHERE " + " AND ".join(conditions) + "\n"

    # Focus on specific concept if provided
    if focus_concept:
        query = """
        MATCH (c:Concept {name: $focus_concept})
        OPTIONAL MATCH (c)-[r:RELATED_TO]-(other:Concept)
        """

    query += """
    RETURN c, r, other
    """

    # Execute query
    with driver.session() as session:
        result = session.run(query, focus_concept=focus_concept)

        # Create graph
        G = nx.Graph()

        # Process results
        for record in result:
            concept = record["c"]
            relationship = record["r"]
            other_concept = record["other"]

            # Add nodes with attributes
            if not G.has_node(concept["name"]):
                G.add_node(concept["name"],
                           type=concept.get("type", "unknown"),
                           document_count=concept.get("document_count", 0))

            if other_concept and not G.has_node(other_concept["name"]):
                G.add_node(other_concept["name"],
                           type=other_concept.get("type", "unknown"),
                           document_count=other_concept.get("document_count", 0))

            # Add edges with attributes
            if relationship and other_concept:
                G.add_edge(concept["name"], other_concept["name"],
                          strength=relationship.get("strength", 0),
                          context=relationship.get("context", ""))

    # Create interactive visualization
    net = Network(height="800px", width="100%", notebook=False)

    # Add nodes with styling based on color_by parameter
    for node, attrs in G.nodes(data=True):
        if color_by == 'type':
            color = get_color_for_type(attrs.get('type', 'unknown'))
        elif color_by == 'count':
            color = get_color_for_count(attrs.get('document_count', 0))
        else:
            color = "#6c8ebf"  # Default color

        size = 10 + min(attrs.get('document_count', 0) * 2, 40)
        net.add_node(node, label=node, title=f"Type: {attrs.get('type', 'unknown')}\nDocuments: {attrs.get('document_count', 0)}",
                    color=color, size=size)

    # Add edges with styling
    for source, target, attrs in G.edges(data=True):
        width = 1 + min(attrs.get('strength', 0) * 5, 10)
        net.add_edge(source, target, width=width, title=f"Strength: {attrs.get('strength', 0)}\nContext: {attrs.get('context', '')}")

    # Configure physics
    net.barnes_hut(gravity=-80000, central_gravity=0.3, spring_length=250)

    # Save to HTML file
    net.save_graph("graph_visualization.html")

    return G
```

#### Concept Clustering

```python
def cluster_concepts(graph, algorithm='louvain', min_cluster_size=3):
    """
    Apply clustering algorithm to group related concepts.

    Args:
        graph (networkx.Graph): The graph to cluster
        algorithm (str): Clustering algorithm to use ('louvain', 'leiden', 'spectral')
        min_cluster_size (int): Minimum number of nodes in a cluster

    Returns:
        dict: Mapping of nodes to cluster IDs
    """
    import community as community_louvain
    import networkx as nx

    if algorithm == 'louvain':
        # Apply Louvain community detection
        partition = community_louvain.best_partition(graph)
    elif algorithm == 'spectral':
        # Apply spectral clustering
        from sklearn.cluster import SpectralClustering

        # Create adjacency matrix
        adjacency = nx.to_numpy_array(graph)

        # Determine number of clusters (heuristic: sqrt of node count)
        n_clusters = max(2, int(np.sqrt(len(graph.nodes))))

        # Apply spectral clustering
        clustering = SpectralClustering(n_clusters=n_clusters,
                                       affinity='precomputed',
                                       assign_labels='discretize')
        labels = clustering.fit_predict(adjacency)

        # Create partition dictionary
        partition = {node: label for node, label in zip(graph.nodes(), labels)}
    else:
        # Default to Louvain
        partition = community_louvain.best_partition(graph)

    # Filter out small clusters
    cluster_sizes = {}
    for cluster_id in partition.values():
        if cluster_id not in cluster_sizes:
            cluster_sizes[cluster_id] = 0
        cluster_sizes[cluster_id] += 1

    # Assign small clusters to a special "unclustered" group
    for node, cluster_id in partition.items():
        if cluster_sizes[cluster_id] < min_cluster_size:
            partition[node] = -1  # -1 represents "unclustered"

    return partition
```

#### Export Functionality

```python
def export_graph(graph, format='png', filename=None, selected_nodes=None):
    """
    Export graph visualization to various formats.

    Args:
        graph (networkx.Graph): The graph to export
        format (str): Export format ('png', 'svg', 'pdf', 'gexf', 'graphml')
        filename (str): Output filename (without extension)
        selected_nodes (list): Subset of nodes to include in export

    Returns:
        str: Path to the exported file
    """
    import networkx as nx
    import matplotlib.pyplot as plt
    import os

    if filename is None:
        filename = f"graph_export_{int(time.time())}"

    # Create subgraph if selected_nodes is provided
    if selected_nodes:
        subgraph = graph.subgraph(selected_nodes)
    else:
        subgraph = graph

    if format in ['png', 'svg', 'pdf']:
        # Visual export using matplotlib
        plt.figure(figsize=(12, 12))

        # Use spring layout for node positioning
        pos = nx.spring_layout(subgraph, k=0.3, iterations=50)

        # Draw nodes
        nx.draw_networkx_nodes(subgraph, pos,
                              node_size=[10 + subgraph.nodes[n].get('document_count', 0) * 2 for n in subgraph.nodes],
                              node_color=[get_color_for_type(subgraph.nodes[n].get('type', 'unknown')) for n in subgraph.nodes])

        # Draw edges with varying width based on strength
        edge_widths = [1 + subgraph.edges[e].get('strength', 0) * 3 for e in subgraph.edges]
        nx.draw_networkx_edges(subgraph, pos, width=edge_widths, alpha=0.7)

        # Draw labels
        nx.draw_networkx_labels(subgraph, pos, font_size=8)

        # Save figure
        plt.axis('off')
        plt.tight_layout()
        plt.savefig(f"{filename}.{format}", format=format, dpi=300)
        plt.close()

        return f"{filename}.{format}"

    elif format == 'gexf':
        # Export to GEXF (Gephi format)
        nx.write_gexf(subgraph, f"{filename}.gexf")
        return f"{filename}.gexf"

    elif format == 'graphml':
        # Export to GraphML
        nx.write_graphml(subgraph, f"{filename}.graphml")
        return f"{filename}.graphml"

    else:
        raise ValueError(f"Unsupported export format: {format}")
```

## 2. Refine Graph Queries

### Problem Statement

The current Neo4j queries need optimization for better performance, more sophisticated relationship scoring, and specialized queries for different use cases.

### Requirements

1. **Query Optimization**
   - Optimize Neo4j queries for better performance
   - Implement proper indexing for frequently queried properties
   - Use parameterized queries to improve security and performance
   - Add query result caching for frequently used queries

2. **Relationship Scoring**
   - Implement more sophisticated relationship scoring algorithms
   - Consider context and semantic similarity in relationship strength
   - Add directional relationships where appropriate
   - Support for weighted path finding

3. **Temporal Aspects**
   - Add temporal tracking to monitor concept evolution
   - Implement versioning for concepts and relationships
   - Support for time-based queries to see concept changes
   - Add trending analysis for concepts over time

4. **Specialized Queries**
   - Develop specialized queries for different use cases
   - Create query templates for common operations
   - Implement query builders for complex dynamic queries
   - Add support for natural language query translation

### Implementation Details

#### Query Optimization

```python
# Neo4j index creation
def create_optimized_indexes():
    """Create optimized indexes for Neo4j."""
    driver = get_neo4j_driver()

    with driver.session() as session:
        # Create index on Concept name (for exact lookups)
        session.run("CREATE INDEX concept_name IF NOT EXISTS FOR (c:Concept) ON (c.name)")

        # Create index on Document title (for exact lookups)
        session.run("CREATE INDEX document_title IF NOT EXISTS FOR (d:Document) ON (d.title)")

        # Create full-text index for concept search
        session.run("""
        CREATE FULLTEXT INDEX concept_fulltext IF NOT EXISTS
        FOR (c:Concept) ON EACH [c.name, c.description]
        OPTIONS {
            indexConfig: {
                `fulltext.analyzer`: 'english'
            }
        }
        """)

        # Create full-text index for document search
        session.run("""
        CREATE FULLTEXT INDEX document_fulltext IF NOT EXISTS
        FOR (d:Document) ON EACH [d.title, d.author, d.summary]
        OPTIONS {
            indexConfig: {
                `fulltext.analyzer`: 'english'
            }
        }
        """)
```

#### Query Caching

```python
class QueryCache:
    """Simple cache for Neo4j query results."""

    def __init__(self, max_size=100, ttl=300):
        """
        Initialize query cache.

        Args:
            max_size (int): Maximum number of cached queries
            ttl (int): Time-to-live in seconds for cache entries
        """
        self.cache = {}
        self.max_size = max_size
        self.ttl = ttl
        self.access_times = {}

    def get(self, query, params=None):
        """Get cached result for query if available."""
        key = self._make_key(query, params)

        if key in self.cache:
            # Check if entry is expired
            timestamp = self.access_times[key]
            if time.time() - timestamp > self.ttl:
                # Entry expired
                del self.cache[key]
                del self.access_times[key]
                return None

            # Update access time
            self.access_times[key] = time.time()
            return self.cache[key]

        return None

    def set(self, query, result, params=None):
        """Cache result for query."""
        key = self._make_key(query, params)

        # Evict oldest entry if cache is full
        if len(self.cache) >= self.max_size:
            oldest_key = min(self.access_times, key=self.access_times.get)
            del self.cache[oldest_key]
            del self.access_times[oldest_key]

        # Store result and access time
        self.cache[key] = result
        self.access_times[key] = time.time()

    def _make_key(self, query, params):
        """Create a unique key for the query and parameters."""
        if params:
            param_str = json.dumps(params, sort_keys=True)
        else:
            param_str = ""

        return f"{query}:{param_str}"

# Initialize global cache
query_cache = QueryCache()

def execute_cached_query(query, params=None, force_refresh=False):
    """Execute Neo4j query with caching."""
    if not force_refresh:
        cached_result = query_cache.get(query, params)
        if cached_result is not None:
            return cached_result

    driver = get_neo4j_driver()
    with driver.session() as session:
        result = session.run(query, params)
        records = [record.data() for record in result]

        # Cache the result
        query_cache.set(query, records, params)

        return records
```

#### Relationship Scoring

```python
def calculate_relationship_strength(concept1, concept2, documents):
    """
    Calculate relationship strength between two concepts based on co-occurrence and semantic similarity.

    Args:
        concept1 (str): First concept name
        concept2 (str): Second concept name
        documents (list): List of documents to analyze

    Returns:
        float: Relationship strength score (0-1)
    """
    # Co-occurrence score
    co_occurrence_count = 0
    total_documents = len(documents)

    for doc in documents:
        if concept1.lower() in doc.text.lower() and concept2.lower() in doc.text.lower():
            # Check if concepts appear close to each other (within 100 words)
            if concepts_are_proximate(doc.text, concept1, concept2, window_size=100):
                co_occurrence_count += 1

    co_occurrence_score = co_occurrence_count / total_documents if total_documents > 0 else 0

    # Semantic similarity score
    semantic_score = calculate_semantic_similarity(concept1, concept2)

    # Combined score (weighted average)
    combined_score = (0.7 * co_occurrence_score) + (0.3 * semantic_score)

    return combined_score

def concepts_are_proximate(text, concept1, concept2, window_size=100):
    """Check if concepts appear close to each other in text."""
    import re

    # Find all occurrences of both concepts
    c1_positions = [m.start() for m in re.finditer(re.escape(concept1.lower()), text.lower())]
    c2_positions = [m.start() for m in re.finditer(re.escape(concept2.lower()), text.lower())]

    # Check if any occurrences are within window_size words of each other
    for pos1 in c1_positions:
        for pos2 in c2_positions:
            # Calculate word distance (approximate)
            word_distance = abs(text[min(pos1, pos2):max(pos1, pos2)].count(' '))
            if word_distance <= window_size:
                return True

    return False

def calculate_semantic_similarity(concept1, concept2):
    """Calculate semantic similarity between concepts using word embeddings."""
    import spacy

    # Load SpaCy model with word vectors
    # Load SpaCy model with word vectors
    # Note: Requires Python <3.13, >=3.9 and spaCy 3.8.5 or compatible version
    # Install with: uv pip install spacy==3.8.5
    # Download model with: python -m spacy download en_core_web_lg
    nlp = spacy.load("en_core_web_lg")

    # Get word vectors
    vec1 = nlp(concept1).vector
    vec2 = nlp(concept2).vector

    # Calculate cosine similarity
    from numpy import dot
    from numpy.linalg import norm

    similarity = dot(vec1, vec2) / (norm(vec1) * norm(vec2))

    return max(0, float(similarity))  # Ensure non-negative
```

#### Specialized Queries

```python
def find_concept_path(start_concept, end_concept, max_depth=3):
    """
    Find paths between two concepts in the knowledge graph.

    Args:
        start_concept (str): Starting concept name
        end_concept (str): Ending concept name
        max_depth (int): Maximum path length to consider

    Returns:
        list: List of paths between concepts
    """
    query = """
    MATCH path = shortestPath((start:Concept {name: $start_name})-[:RELATED_TO*1..{max_depth}]-(end:Concept {name: $end_name}))
    RETURN path, length(path) AS path_length
    ORDER BY path_length
    LIMIT 5
    """

    params = {
        "start_name": start_concept,
        "end_name": end_concept,
        "max_depth": max_depth
    }

    driver = get_neo4j_driver()
    with driver.session() as session:
        result = session.run(query, params)

        paths = []
        for record in result:
            path = record["path"]
            path_data = {
                "length": record["path_length"],
                "nodes": [node["name"] for node in path.nodes],
                "relationships": [rel["strength"] for rel in path.relationships]
            }
            paths.append(path_data)

        return paths

def find_concept_community(concept_name, max_distance=2):
    """
    Find the community of concepts around a given concept.

    Args:
        concept_name (str): Central concept name
        max_distance (int): Maximum distance from central concept

    Returns:
        dict: Community structure with nodes and relationships
    """
    query = """
    MATCH (center:Concept {name: $concept_name})
    CALL {
        MATCH path = (center)-[:RELATED_TO*1..{max_distance}]-(related:Concept)
        RETURN related, length(path) AS distance
    }
    WITH center, related, min(distance) AS min_distance
    MATCH (related)-[r:RELATED_TO]-(other:Concept)
    WHERE other.name <> $concept_name AND (other)-[:RELATED_TO*1..{max_distance}]-(center)
    RETURN center, collect(distinct {name: related.name, distance: min_distance}) AS related_concepts,
           collect(distinct {from: related.name, to: other.name, strength: r.strength}) AS relationships
    """

    params = {
        "concept_name": concept_name,
        "max_distance": max_distance
    }

    return execute_cached_query(query, params)
```

## 3. LLM Integration for Enhanced Concept Analysis

### Problem Statement

The current system lacks advanced natural language understanding capabilities that could be provided by integrating with Large Language Models (LLMs). This integration would enhance concept extraction, relationship analysis, and provide more sophisticated semantic understanding.

### Requirements

1. **LLM Integration Architecture**
   - Design a flexible architecture for integrating with various LLM endpoints
   - Support for local LLM servers (e.g., LM Studio, Ollama) and cloud APIs
   - Implement proper error handling and fallback mechanisms
   - Create a configuration system for LLM endpoint settings

2. **Enhanced Concept Extraction**
   - Use LLMs to extract concepts with better semantic understanding
   - Implement entity recognition for specialized domains
   - Support for identifying complex relationships between concepts
   - Add capability to extract hierarchical concept structures

3. **Semantic Analysis**
   - Implement semantic similarity calculations using LLMs
   - Add sentiment analysis for concept contexts
   - Support for identifying concept attributes and properties
   - Implement contextual understanding of concepts

4. **Query Enhancement**
   - Use LLMs to translate natural language queries to graph queries
   - Implement semantic search capabilities
   - Add support for answering complex questions about the knowledge graph
   - Create summarization capabilities for graph subsets

### Implementation Details

#### LLM Integration Architecture

```python
from typing import List, Dict, Any, Optional, Union, Callable
import requests
import json
import os
import logging
from abc import ABC, abstractmethod

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class LLMProvider(ABC):
    """Abstract base class for LLM providers."""

    @abstractmethod
    def generate(self, prompt: str, **kwargs) -> str:
        """Generate text based on prompt."""
        pass

    @abstractmethod
    def generate_batch(self, prompts: List[str], **kwargs) -> List[str]:
        """Generate text for multiple prompts."""
        pass

    @abstractmethod
    def get_embeddings(self, texts: List[str], **kwargs) -> List[List[float]]:
        """Get embeddings for texts."""
        pass

class OpenAICompatibleProvider(LLMProvider):
    """Provider for OpenAI-compatible API endpoints (including LM Studio)."""

    def __init__(self,
                 api_base: str = "http://localhost:1234/v1",
                 api_key: str = "dummy-key",
                 model: str = "local-model",
                 embedding_model: str = None,
                 temperature: float = 0.0,
                 max_tokens: int = 1000,
                 timeout: int = 60):
        """
        Initialize OpenAI-compatible provider.

        Args:
            api_base: Base URL for API
            api_key: API key (can be dummy for local servers)
            model: Model name to use
            embedding_model: Model for embeddings (if different)
            temperature: Temperature for generation
            max_tokens: Maximum tokens to generate
            timeout: Request timeout in seconds
        """
        self.api_base = api_base
        self.api_key = api_key
        self.model = model
        self.embedding_model = embedding_model or model
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.timeout = timeout

        # Test connection
        self._test_connection()

    def _test_connection(self):
        """Test connection to the API endpoint."""
        try:
            # Simple test request
            response = requests.get(
                f"{self.api_base}/models",
                headers={"Authorization": f"Bearer {self.api_key}"},
                timeout=self.timeout
            )

            if response.status_code == 200:
                logger.info(f"Successfully connected to LLM API at {self.api_base}")
            else:
                logger.warning(f"Connected to {self.api_base} but received status code {response.status_code}")

        except requests.exceptions.RequestException as e:
            logger.warning(f"Failed to connect to LLM API at {self.api_base}: {e}")

    def generate(self, prompt: str, **kwargs) -> str:
        """
        Generate text based on prompt using chat completions API.

        Args:
            prompt: Text prompt
            **kwargs: Additional parameters to override defaults

        Returns:
            Generated text
        """
        # Prepare messages
        messages = [
            {"role": "system", "content": kwargs.get("system_prompt", "You are a helpful assistant.")},
            {"role": "user", "content": prompt}
        ]

        # Prepare request payload
        payload = {
            "model": kwargs.get("model", self.model),
            "messages": messages,
            "temperature": kwargs.get("temperature", self.temperature),
            "max_tokens": kwargs.get("max_tokens", self.max_tokens)
        }

        # Add any additional parameters
        for key, value in kwargs.items():
            if key not in ["system_prompt", "model", "temperature", "max_tokens"]:
                payload[key] = value

        try:
            # Make API request
            response = requests.post(
                f"{self.api_base}/chat/completions",
                headers={"Authorization": f"Bearer {self.api_key}"},
                json=payload,
                timeout=self.timeout
            )

            response.raise_for_status()
            result = response.json()

            # Extract generated text
            if "choices" in result and len(result["choices"]) > 0:
                message = result["choices"][0]["message"]

                # Handle different response formats (some models use content, others use reasoning_content)
                content = message.get("content", "")
                reasoning_content = message.get("reasoning_content", "")

                # Return whichever content field has data
                return content if content else reasoning_content
            else:
                logger.warning("No choices in LLM response")
                return ""

        except requests.exceptions.RequestException as e:
            logger.error(f"Error calling LLM API: {e}")
            if hasattr(e, 'response') and e.response is not None:
                logger.error(f"Response status code: {e.response.status_code}")
                logger.error(f"Response text: {e.response.text}")
            return f"Error: {str(e)}"

    def generate_batch(self, prompts: List[str], **kwargs) -> List[str]:
        """
        Generate text for multiple prompts.

        Args:
            prompts: List of text prompts
            **kwargs: Additional parameters

        Returns:
            List of generated texts
        """
        return [self.generate(prompt, **kwargs) for prompt in prompts]

    def get_embeddings(self, texts: List[str], **kwargs) -> List[List[float]]:
        """
        Get embeddings for texts.

        Args:
            texts: List of texts to embed
            **kwargs: Additional parameters

        Returns:
            List of embedding vectors
        """
        model = kwargs.get("model", self.embedding_model)

        try:
            # Make API request
            response = requests.post(
                f"{self.api_base}/embeddings",
                headers={"Authorization": f"Bearer {self.api_key}"},
                json={"model": model, "input": texts},
                timeout=self.timeout
            )

            response.raise_for_status()
            result = response.json()

            # Extract embeddings
            if "data" in result:
                return [item["embedding"] for item in result["data"]]
            else:
                logger.warning("No embeddings in response")
                return [[0.0] for _ in texts]  # Return dummy embeddings

        except requests.exceptions.RequestException as e:
            logger.error(f"Error getting embeddings: {e}")
            # Return dummy embeddings in case of error
            return [[0.0] for _ in texts]

class LLMManager:
    """Manager for LLM providers with fallback capabilities."""

    def __init__(self, primary_provider: LLMProvider, fallback_provider: Optional[LLMProvider] = None):
        """
        Initialize LLM manager.

        Args:
            primary_provider: Primary LLM provider
            fallback_provider: Fallback provider (optional)
        """
        self.primary_provider = primary_provider
        self.fallback_provider = fallback_provider

    def generate(self, prompt: str, **kwargs) -> str:
        """
        Generate text with fallback capability.

        Args:
            prompt: Text prompt
            **kwargs: Additional parameters

        Returns:
            Generated text
        """
        try:
            return self.primary_provider.generate(prompt, **kwargs)
        except Exception as e:
            logger.warning(f"Primary provider failed: {e}")
            if self.fallback_provider:
                logger.info("Trying fallback provider")
                return self.fallback_provider.generate(prompt, **kwargs)
            else:
                raise

    def generate_batch(self, prompts: List[str], **kwargs) -> List[str]:
        """Generate text for multiple prompts with fallback."""
        try:
            return self.primary_provider.generate_batch(prompts, **kwargs)
        except Exception as e:
            logger.warning(f"Primary provider failed for batch: {e}")
            if self.fallback_provider:
                logger.info("Trying fallback provider for batch")
                return self.fallback_provider.generate_batch(prompts, **kwargs)
            else:
                raise

    def get_embeddings(self, texts: List[str], **kwargs) -> List[List[float]]:
        """Get embeddings with fallback."""
        try:
            return self.primary_provider.get_embeddings(texts, **kwargs)
        except Exception as e:
            logger.warning(f"Primary provider failed for embeddings: {e}")
            if self.fallback_provider:
                logger.info("Trying fallback provider for embeddings")
                return self.fallback_provider.get_embeddings(texts, **kwargs)
            else:
                raise

# Factory function to create LLM provider based on configuration
def create_llm_provider(config: Dict[str, Any]) -> LLMProvider:
    """
    Create LLM provider based on configuration.

    Args:
        config: Configuration dictionary

    Returns:
        LLM provider instance
    """
    provider_type = config.get("type", "openai-compatible")

    if provider_type == "openai-compatible":
        return OpenAICompatibleProvider(
            api_base=config.get("api_base", "http://localhost:1234/v1"),
            api_key=config.get("api_key", "dummy-key"),
            model=config.get("model", "local-model"),
            embedding_model=config.get("embedding_model"),
            temperature=config.get("temperature", 0.0),
            max_tokens=config.get("max_tokens", 1000),
            timeout=config.get("timeout", 60)
        )
    else:
        raise ValueError(f"Unsupported provider type: {provider_type}")
```

#### Enhanced Concept Extraction with LLMs

```python
def extract_concepts_with_llm(text: str, llm_manager: LLMManager) -> List[Dict[str, Any]]:
    """
    Extract concepts from text using LLM.

    Args:
        text: Input text
        llm_manager: LLM manager instance

    Returns:
        List of extracted concepts with metadata
    """
    # Prepare prompt for concept extraction
    prompt = f"""
    Extract key concepts from the following text. For each concept, identify:
    1. The concept name
    2. The concept type (PERSON, ORGANIZATION, LOCATION, TECHNOLOGY, PROCESS, ABSTRACT, etc.)
    3. A brief description based on the context
    4. Related concepts mentioned in the text

    Format the output as a JSON array of objects with the following structure:
    [
        {{
            "name": "concept_name",
            "type": "concept_type",
            "description": "brief_description",
            "related_concepts": ["related1", "related2"]
        }}
    ]

    TEXT:
    {text}
    """

    # Generate response
    response = llm_manager.generate(prompt, system_prompt="You are an expert in knowledge extraction and ontology creation.")

    # Parse JSON from response
    try:
        # Find JSON in the response (it might be embedded in text)
        json_start = response.find('[')
        json_end = response.rfind(']') + 1

        if json_start >= 0 and json_end > json_start:
            json_str = response[json_start:json_end]
            concepts = json.loads(json_str)
            return concepts
        else:
            logger.warning("Could not find JSON array in LLM response")
            return []
    except json.JSONDecodeError:
        logger.warning("Failed to parse JSON from LLM response")
        return []

def analyze_concept_relationships(concepts: List[Dict[str, Any]], llm_manager: LLMManager) -> List[Dict[str, Any]]:
    """
    Analyze relationships between concepts using LLM.

    Args:
        concepts: List of concepts
        llm_manager: LLM manager instance

    Returns:
        List of relationships with metadata
    """
    # Extract concept names
    concept_names = [concept["name"] for concept in concepts]

    # Prepare prompt for relationship analysis
    prompt = f"""
    Analyze the relationships between the following concepts:
    {', '.join(concept_names)}

    For each pair of related concepts, identify:
    1. The source concept
    2. The target concept
    3. The relationship type (e.g., IS_PART_OF, DEPENDS_ON, INFLUENCES, SIMILAR_TO, etc.)
    4. The relationship strength (a value between 0 and 1)
    5. A brief description of the relationship

    Format the output as a JSON array of objects with the following structure:
    [
        {{
            "source": "source_concept",
            "target": "target_concept",
            "type": "relationship_type",
            "strength": 0.8,
            "description": "brief_description"
        }}
    ]

    Only include relationships that are meaningful and relevant.
    """

    # Generate response
    response = llm_manager.generate(prompt, system_prompt="You are an expert in knowledge graph construction and relationship analysis.")

    # Parse JSON from response
    try:
        # Find JSON in the response
        json_start = response.find('[')
        json_end = response.rfind(']') + 1

        if json_start >= 0 and json_end > json_start:
            json_str = response[json_start:json_end]
            relationships = json.loads(json_str)
            return relationships
        else:
            logger.warning("Could not find JSON array in LLM response")
            return []
    except json.JSONDecodeError:
        logger.warning("Failed to parse JSON from LLM response")
        return []
```

#### SpaCy Integration with LLMs

```python
import spacy
from spacy.language import Language
from spacy.tokens import Doc, Span
from typing import List, Dict, Any, Optional, Union, Callable
import numpy as np

# Register custom extension attributes
Doc.set_extension("llm_concepts", default=None, force=True)
Doc.set_extension("llm_summary", default=None, force=True)
Span.set_extension("sentiment", default=None, force=True)

@Language.factory("llm_concept_extractor")
class LLMConceptExtractor:
    """spaCy pipeline component for extracting concepts using LLM."""

    def __init__(self, nlp: Language, name: str, llm_manager: LLMManager):
        """
        Initialize component.

        Args:
            nlp: spaCy Language object
            name: Component name
            llm_manager: LLM manager instance
        """
        self.nlp = nlp
        self.name = name
        self.llm_manager = llm_manager

    def __call__(self, doc: Doc) -> Doc:
        """
        Process document to extract concepts.

        Args:
            doc: spaCy Doc object

        Returns:
            Processed Doc with concepts
        """
        # Skip processing for very short texts
        if len(doc.text) < 50:
            doc._.llm_concepts = []
            return doc

        # Extract concepts using LLM
        concepts = extract_concepts_with_llm(doc.text, self.llm_manager)

        # Store concepts in document extension
        doc._.llm_concepts = concepts

        return doc

@Language.factory("llm_sentiment_analyzer")
class LLMSentimentAnalyzer:
    """spaCy pipeline component for sentiment analysis using LLM."""

    def __init__(self, nlp: Language, name: str, llm_manager: LLMManager):
        """
        Initialize component.

        Args:
            nlp: spaCy Language object
            name: Component name
            llm_manager: LLM manager instance
        """
        self.nlp = nlp
        self.name = name
        self.llm_manager = llm_manager

    def __call__(self, doc: Doc) -> Doc:
        """
        Process document to analyze sentiment.

        Args:
            doc: spaCy Doc object

        Returns:
            Processed Doc with sentiment
        """
        # Skip processing for very short texts
        if len(doc.text) < 10:
            return doc

        # Analyze sentiment for each sentence
        for sent in doc.sents:
            # Prepare prompt
            prompt = f"Analyze the sentiment of this text: '{sent.text}'\nRespond with only one word: POSITIVE, NEGATIVE, or NEUTRAL."

            # Generate response
            response = self.llm_manager.generate(prompt, max_tokens=50)

            # Extract sentiment
            if "POSITIVE" in response:
                sent._.sentiment = "POSITIVE"
            elif "NEGATIVE" in response:
                sent._.sentiment = "NEGATIVE"
            elif "NEUTRAL" in response:
                sent._.sentiment = "NEUTRAL"
            else:
                sent._.sentiment = "UNKNOWN"

        return doc

def create_nlp_pipeline_with_llm(llm_config: Dict[str, Any]) -> Language:
    """
    Create spaCy pipeline with LLM integration.

    Args:
        llm_config: LLM configuration

    Returns:
        Configured spaCy Language object
    """
    # Note: Requires Python <3.13, >=3.9 and spaCy 3.8.5 or compatible version
    # Install with: uv pip install spacy==3.8.5
    # Download model with: python -m spacy download en_core_web_lg

    # Load base spaCy model
    nlp = spacy.load("en_core_web_lg")

    # Create LLM provider and manager
    llm_provider = create_llm_provider(llm_config)
    llm_manager = LLMManager(llm_provider)

    # Add custom components
    nlp.add_pipe("llm_concept_extractor", config={"llm_manager": llm_manager})
    nlp.add_pipe("llm_sentiment_analyzer", config={"llm_manager": llm_manager})

    return nlp

# Example usage with LM Studio's Phi-4-mini-reasoning-MLX-4bit model
llm_config = {
    "type": "openai-compatible",
    "api_base": "http://192.168.1.21:1234/v1",
    "api_key": "dummy-key",
    "model": "lmstudio-community/Phi-4-mini-reasoning-MLX-4bit",
    "temperature": 0.1,
    "max_tokens": 500
}

# Create pipeline (commented out to avoid execution during import)
# nlp = create_nlp_pipeline_with_llm(llm_config)
```

#### Natural Language to Graph Query Translation

```python
def translate_nl_to_graph_query(question: str, llm_manager: LLMManager) -> Dict[str, Any]:
    """
    Translate natural language question to graph query.

    Args:
        question: Natural language question
        llm_manager: LLM manager instance

    Returns:
        Dictionary with query information
    """
    # Prepare prompt for query translation
    prompt = f"""
    Translate the following natural language question into a Cypher query for Neo4j:

    QUESTION: {question}

    The knowledge graph has the following structure:
    - Nodes with label 'Concept' have properties: name, type, description
    - Nodes with label 'Document' have properties: title, author, summary, content
    - Relationships between Concepts: RELATED_TO with properties: strength, context
    - Relationships between Concepts and Documents: APPEARS_IN with properties: count, context

    Format your response as a JSON object with the following structure:
    {{
        "cypher_query": "MATCH ... RETURN ...",
        "parameters": {{"param1": "value1", ...}},
        "explanation": "Brief explanation of the query"
    }}
    """

    # Generate response
    response = llm_manager.generate(prompt, system_prompt="You are an expert in Neo4j and Cypher query language.")

    # Parse JSON from response
    try:
        # Find JSON in the response
        json_start = response.find('{')
        json_end = response.rfind('}') + 1

        if json_start >= 0 and json_end > json_start:
            json_str = response[json_start:json_end]
            query_info = json.loads(json_str)
            return query_info
        else:
            logger.warning("Could not find JSON object in LLM response")
            return {
                "cypher_query": "",
                "parameters": {},
                "explanation": "Failed to parse query from LLM response"
            }
    except json.JSONDecodeError:
        logger.warning("Failed to parse JSON from LLM response")
        return {
            "cypher_query": "",
            "parameters": {},
            "explanation": "Failed to parse query from LLM response"
        }
```

## Implementation Timeline

### Week 1
- Implement enhanced visualization script with filtering options
- Create optimized Neo4j indexes
- Develop query caching mechanism

### Week 2
- Implement concept clustering algorithms
- Develop relationship scoring improvements
- Create specialized query templates

### Week 3
- Implement LLM integration architecture
- Create SpaCy pipeline with LLM components
- Develop concept extraction with LLMs

### Week 4
- Implement interactive graph exploration features
- Add export functionality for different formats
- Integrate all components into cohesive system

## Testing Strategy

1. **Unit Tests**
   - Test relationship scoring with known concept pairs
   - Verify query caching with repeated queries
   - Test clustering algorithms with sample graphs
   - Test LLM provider with mock responses

2. **Integration Tests**
   - Test visualization with real Neo4j data
   - Verify export functionality with different formats
   - Test specialized queries with complex graph structures
   - Test SpaCy pipeline with LLM components
   - Verify concept extraction with different text types

3. **Performance Tests**
   - Benchmark query performance before and after optimization
   - Test visualization rendering with large graphs
   - Measure memory usage during complex operations
   - Evaluate LLM response times and throughput
   - Test fallback mechanisms under failure conditions

## Docker Considerations

All changes will follow the immutable container principle:

1. Develop and test changes locally
2. Update Dockerfile and requirements.txt as needed
3. Rebuild container to apply changes
4. Never modify running containers directly

## Success Criteria

1. 50% improvement in Neo4j query performance for common operations
2. Successful visualization of graphs with 1000+ nodes
3. Accurate concept clustering with meaningful groupings
4. Successful export to at least 3 different visualization formats
5. Successful integration with at least 2 different LLM providers
6. 80% accuracy in concept extraction compared to manual extraction
7. Successful natural language to graph query translation for common question types

## Environment Requirements

1. **Python Environment**
   - Python version: <3.13, >=3.9 (spaCy 3.8.5 compatibility requirement)
   - Package management: UV for consistent dependency management
   - Virtual environment: Isolated environment for each project

2. **LLM Integration**
   - Local LLM server: LM Studio with Phi-4-mini-reasoning-MLX-4bit model
   - API compatibility: OpenAI-compatible API format
   - Fallback options: Support for multiple LLM providers

3. **Docker Considerations**
   - Base image: Python 3.12 (for spaCy compatibility)
   - Volume mounting: Mount LLM configuration for flexibility
   - Network configuration: Allow container to access LLM endpoints
