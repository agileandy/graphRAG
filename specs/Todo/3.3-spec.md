# Specification: Strengthen Asynchronous Processing (Phase 3.3)

## Overview

This specification outlines the improvements needed for asynchronous processing in the GraphRAG system, focusing on robust job management, scalability improvements, and resource optimization to handle large document sets efficiently.

## 1. Robust Job Management

### Problem Statement

The current asynchronous processing system lacks detailed status reporting, job prioritization, recovery mechanisms for failed operations, and proper resource cleanup during job cancellation.

### Requirements

1. **Enhanced Job Tracking**
   - Implement detailed job status reporting with progress indicators
   - Create persistent job storage for tracking across system restarts
   - Add job history with detailed execution logs
   - Implement job dependency tracking for complex workflows

2. **Job Prioritization**
   - Add priority levels for different job types
   - Implement queue management based on job priority
   - Support for preemption of lower-priority jobs
   - Add deadline-based scheduling for time-sensitive operations

3. **Job Recovery**
   - Implement automatic recovery for failed operations
   - Add checkpointing for long-running jobs
   - Create resumable job execution from checkpoints
   - Implement failure analysis and reporting

4. **Job Cancellation**
   - Add proper resource cleanup during job cancellation
   - Implement graceful termination of running processes
   - Support for partial results from cancelled jobs
   - Add audit logging for cancelled operations

### Implementation Details

#### Enhanced Job Tracking

```python
class JobStatus(enum.Enum):
    """Enum for job status values."""
    PENDING = "pending"
    RUNNING = "running"
    PAUSED = "paused"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

class JobManager:
    """Manager for asynchronous job tracking and execution."""
    
    def __init__(self, db_path="jobs.sqlite"):
        """Initialize job manager with database connection."""
        self.db_path = db_path
        self._init_db()
        self.active_jobs = {}  # In-memory tracking of active jobs
    
    def _init_db(self):
        """Initialize the job database."""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Create jobs table if it doesn't exist
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS jobs (
            job_id TEXT PRIMARY KEY,
            job_type TEXT NOT NULL,
            status TEXT NOT NULL,
            priority INTEGER DEFAULT 0,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            started_at TIMESTAMP,
            completed_at TIMESTAMP,
            progress REAL DEFAULT 0,
            result TEXT,
            error TEXT,
            parameters TEXT
        )
        ''')
        
        # Create job_logs table for detailed execution logs
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS job_logs (
            log_id INTEGER PRIMARY KEY AUTOINCREMENT,
            job_id TEXT NOT NULL,
            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            level TEXT NOT NULL,
            message TEXT NOT NULL,
            FOREIGN KEY (job_id) REFERENCES jobs (job_id)
        )
        ''')
        
        conn.commit()
        conn.close()
    
    def create_job(self, job_type, parameters, priority=0):
        """
        Create a new job in the system.
        
        Args:
            job_type (str): Type of job (e.g., 'add_document', 'add_folder')
            parameters (dict): Job parameters
            priority (int): Job priority (higher values = higher priority)
            
        Returns:
            str: Job ID
        """
        job_id = str(uuid.uuid4())
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
        INSERT INTO jobs (job_id, job_type, status, priority, parameters)
        VALUES (?, ?, ?, ?, ?)
        ''', (job_id, job_type, JobStatus.PENDING.value, priority, json.dumps(parameters)))
        
        conn.commit()
        conn.close()
        
        self.log_job_event(job_id, "INFO", f"Job created: {job_type}")
        
        return job_id
    
    def get_job_status(self, job_id):
        """
        Get the current status of a job.
        
        Args:
            job_id (str): Job ID
            
        Returns:
            dict: Job status information
        """
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        cursor.execute('''
        SELECT * FROM jobs WHERE job_id = ?
        ''', (job_id,))
        
        job = cursor.fetchone()
        conn.close()
        
        if not job:
            return None
        
        # Convert to dict
        job_dict = dict(job)
        
        # Parse JSON fields
        if job_dict['parameters']:
            job_dict['parameters'] = json.loads(job_dict['parameters'])
        
        # Add in-memory progress if available
        if job_id in self.active_jobs and hasattr(self.active_jobs[job_id], 'progress'):
            job_dict['progress'] = self.active_jobs[job_id].progress
        
        return job_dict
    
    def update_job_status(self, job_id, status, progress=None, result=None, error=None):
        """
        Update the status of a job.
        
        Args:
            job_id (str): Job ID
            status (JobStatus): New job status
            progress (float, optional): Job progress (0-100)
            result (dict, optional): Job result data
            error (str, optional): Error message if job failed
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        updates = ["status = ?"]
        params = [status.value]
        
        if status == JobStatus.RUNNING and progress is None:
            updates.append("started_at = CURRENT_TIMESTAMP")
        
        if status in [JobStatus.COMPLETED, JobStatus.FAILED, JobStatus.CANCELLED]:
            updates.append("completed_at = CURRENT_TIMESTAMP")
        
        if progress is not None:
            updates.append("progress = ?")
            params.append(progress)
        
        if result is not None:
            updates.append("result = ?")
            params.append(json.dumps(result))
        
        if error is not None:
            updates.append("error = ?")
            params.append(error)
        
        query = f'''
        UPDATE jobs SET {", ".join(updates)}
        WHERE job_id = ?
        '''
        params.append(job_id)
        
        cursor.execute(query, params)
        conn.commit()
        conn.close()
        
        self.log_job_event(job_id, "INFO", f"Job status updated to {status.value}")
    
    def log_job_event(self, job_id, level, message):
        """
        Log an event for a job.
        
        Args:
            job_id (str): Job ID
            level (str): Log level (INFO, WARNING, ERROR)
            message (str): Log message
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
        INSERT INTO job_logs (job_id, level, message)
        VALUES (?, ?, ?)
        ''', (job_id, level, message))
        
        conn.commit()
        conn.close()
    
    def get_job_logs(self, job_id):
        """
        Get logs for a specific job.
        
        Args:
            job_id (str): Job ID
            
        Returns:
            list: Job logs
        """
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        cursor.execute('''
        SELECT * FROM job_logs
        WHERE job_id = ?
        ORDER BY timestamp
        ''', (job_id,))
        
        logs = [dict(row) for row in cursor.fetchall()]
        conn.close()
        
        return logs
    
    def list_jobs(self, status=None, job_type=None, limit=100):
        """
        List jobs with optional filtering.
        
        Args:
            status (JobStatus, optional): Filter by job status
            job_type (str, optional): Filter by job type
            limit (int): Maximum number of jobs to return
            
        Returns:
            list: List of jobs
        """
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        query = "SELECT * FROM jobs"
        params = []
        
        conditions = []
        if status:
            conditions.append("status = ?")
            params.append(status.value)
        
        if job_type:
            conditions.append("job_type = ?")
            params.append(job_type)
        
        if conditions:
            query += " WHERE " + " AND ".join(conditions)
        
        query += " ORDER BY created_at DESC LIMIT ?"
        params.append(limit)
        
        cursor.execute(query, params)
        
        jobs = [dict(row) for row in cursor.fetchall()]
        conn.close()
        
        # Parse JSON fields
        for job in jobs:
            if job['parameters']:
                job['parameters'] = json.loads(job['parameters'])
        
        return jobs
    
    def cancel_job(self, job_id):
        """
        Cancel a running or pending job.
        
        Args:
            job_id (str): Job ID
            
        Returns:
            bool: True if job was cancelled, False otherwise
        """
        job = self.get_job_status(job_id)
        
        if not job:
            return False
        
        if job['status'] in [JobStatus.COMPLETED.value, JobStatus.FAILED.value, JobStatus.CANCELLED.value]:
            return False
        
        # If job is running and in active_jobs, attempt to cancel it
        if job_id in self.active_jobs:
            job_executor = self.active_jobs[job_id]
            if hasattr(job_executor, 'cancel') and callable(job_executor.cancel):
                job_executor.cancel()
        
        # Update job status
        self.update_job_status(job_id, JobStatus.CANCELLED, error="Job cancelled by user")
        
        # Clean up resources
        self._cleanup_job_resources(job_id, job['job_type'], job['parameters'])
        
        return True
    
    def _cleanup_job_resources(self, job_id, job_type, parameters):
        """
        Clean up resources associated with a job.
        
        Args:
            job_id (str): Job ID
            job_type (str): Type of job
            parameters (dict): Job parameters
        """
        # Implement resource cleanup based on job type
        if job_type == 'add_document':
            # Clean up temporary files
            if 'temp_path' in parameters and os.path.exists(parameters['temp_path']):
                try:
                    os.remove(parameters['temp_path'])
                    self.log_job_event(job_id, "INFO", f"Cleaned up temporary file: {parameters['temp_path']}")
                except Exception as e:
                    self.log_job_event(job_id, "ERROR", f"Failed to clean up temporary file: {str(e)}")
        
        # Remove from active jobs
        if job_id in self.active_jobs:
            del self.active_jobs[job_id]
```

#### Job Prioritization

```python
class PriorityJobQueue:
    """Priority queue for job execution."""
    
    def __init__(self, job_manager):
        """
        Initialize priority job queue.
        
        Args:
            job_manager (JobManager): Job manager instance
        """
        self.job_manager = job_manager
        self.queue = PriorityQueue()
        self.lock = threading.Lock()
        self.processing = False
    
    def add_job(self, job_id, priority):
        """
        Add a job to the queue.
        
        Args:
            job_id (str): Job ID
            priority (int): Job priority (higher values = higher priority)
        """
        with self.lock:
            # Invert priority for PriorityQueue (which puts lowest values first)
            self.queue.put((-priority, job_id))
    
    def start_processing(self, num_workers=3):
        """
        Start processing jobs from the queue.
        
        Args:
            num_workers (int): Number of worker threads
        """
        if self.processing:
            return
        
        self.processing = True
        
        # Start worker threads
        self.workers = []
        for _ in range(num_workers):
            worker = threading.Thread(target=self._worker_loop)
            worker.daemon = True
            worker.start()
            self.workers.append(worker)
    
    def stop_processing(self):
        """Stop processing jobs."""
        self.processing = False
        
        # Wait for workers to finish
        for worker in self.workers:
            worker.join(timeout=1.0)
    
    def _worker_loop(self):
        """Worker thread loop for processing jobs."""
        while self.processing:
            try:
                # Get job from queue with timeout
                try:
                    _, job_id = self.queue.get(timeout=1.0)
                except Empty:
                    continue
                
                # Get job details
                job = self.job_manager.get_job_status(job_id)
                
                if not job or job['status'] != JobStatus.PENDING.value:
                    self.queue.task_done()
                    continue
                
                # Execute job
                self._execute_job(job_id, job['job_type'], job['parameters'])
                
                self.queue.task_done()
            
            except Exception as e:
                logger.error(f"Error in worker thread: {str(e)}")
                time.sleep(1.0)  # Avoid tight loop on error
    
    def _execute_job(self, job_id, job_type, parameters):
        """
        Execute a job.
        
        Args:
            job_id (str): Job ID
            job_type (str): Type of job
            parameters (dict): Job parameters
        """
        # Update job status to running
        self.job_manager.update_job_status(job_id, JobStatus.RUNNING)
        
        try:
            # Create job executor based on job type
            if job_type == 'add_document':
                executor = DocumentAdditionExecutor(job_id, parameters, self.job_manager)
            elif job_type == 'add_folder':
                executor = FolderAdditionExecutor(job_id, parameters, self.job_manager)
            else:
                raise ValueError(f"Unknown job type: {job_type}")
            
            # Store executor for potential cancellation
            self.job_manager.active_jobs[job_id] = executor
            
            # Execute job
            result = executor.execute()
            
            # Update job status to completed
            self.job_manager.update_job_status(job_id, JobStatus.COMPLETED, 
                                              progress=100.0, result=result)
        
        except Exception as e:
            # Log error and update job status
            logger.error(f"Job {job_id} failed: {str(e)}")
            self.job_manager.log_job_event(job_id, "ERROR", f"Job failed: {str(e)}")
            self.job_manager.update_job_status(job_id, JobStatus.FAILED, 
                                              error=str(e))
        
        finally:
            # Remove from active jobs
            if job_id in self.job_manager.active_jobs:
                del self.job_manager.active_jobs[job_id]
```

#### Job Recovery

```python
class CheckpointableJob:
    """Base class for jobs that support checkpointing."""
    
    def __init__(self, job_id, parameters, job_manager):
        """
        Initialize checkpointable job.
        
        Args:
            job_id (str): Job ID
            parameters (dict): Job parameters
            job_manager (JobManager): Job manager instance
        """
        self.job_id = job_id
        self.parameters = parameters
        self.job_manager = job_manager
        self.progress = 0.0
        self.cancelled = False
        self.checkpoints = []
    
    def execute(self):
        """Execute the job with checkpointing."""
        raise NotImplementedError("Subclasses must implement execute()")
    
    def cancel(self):
        """Mark the job as cancelled."""
        self.cancelled = True
    
    def create_checkpoint(self, state):
        """
        Create a checkpoint with the current state.
        
        Args:
            state (dict): Current job state
        """
        checkpoint = {
            'timestamp': time.time(),
            'progress': self.progress,
            'state': state
        }
        
        self.checkpoints.append(checkpoint)
        
        # Save checkpoint to database
        self.job_manager.log_job_event(
            self.job_id, 
            "INFO", 
            f"Checkpoint created at {self.progress:.1f}% progress"
        )
        
        # Update job progress
        self.job_manager.update_job_status(
            self.job_id,
            JobStatus.RUNNING,
            progress=self.progress
        )
    
    def get_latest_checkpoint(self):
        """
        Get the latest checkpoint.
        
        Returns:
            dict: Latest checkpoint or None if no checkpoints exist
        """
        if not self.checkpoints:
            return None
        
        return self.checkpoints[-1]
    
    def resume_from_checkpoint(self):
        """
        Resume execution from the latest checkpoint.
        
        Returns:
            bool: True if resumed from checkpoint, False otherwise
        """
        checkpoint = self.get_latest_checkpoint()
        
        if not checkpoint:
            return False
        
        self.progress = checkpoint['progress']
        
        self.job_manager.log_job_event(
            self.job_id,
            "INFO",
            f"Resuming from checkpoint at {self.progress:.1f}% progress"
        )
        
        return True
```

## 2. Scalability Improvements

### Problem Statement

The current system lacks parallel processing capabilities, resource monitoring, backpressure mechanisms, and distributed processing options for handling large document sets efficiently.

### Requirements

1. **Worker Pools**
   - Implement worker pools for parallel processing
   - Add configurable thread/process pool sizes
   - Support for different worker types for specialized tasks
   - Implement work stealing for better load balancing

2. **Resource Monitoring**
   - Add resource monitoring to prevent system overload
   - Implement adaptive throttling based on system load
   - Create resource usage reporting for administrators
   - Add alerts for resource constraints

3. **Backpressure Mechanisms**
   - Implement backpressure for high-volume operations
   - Add rate limiting for document processing
   - Create flow control between processing stages
   - Support for graceful degradation under load

4. **Distributed Processing**
   - Design for potential distributed processing
   - Implement job partitioning for distributed execution
   - Add support for work distribution across nodes
   - Create synchronization mechanisms for distributed state

### Implementation Details

#### Worker Pools

```python
class WorkerPool:
    """Thread pool for parallel job execution."""
    
    def __init__(self, max_workers=None, name="default"):
        """
        Initialize worker pool.
        
        Args:
            max_workers (int, optional): Maximum number of worker threads
            name (str): Pool name for identification
        """
        self.name = name
        
        # Determine default max_workers based on CPU count
        if max_workers is None:
            max_workers = min(32, (os.cpu_count() or 1) + 4)
        
        self.max_workers = max_workers
        self.executor = concurrent.futures.ThreadPoolExecutor(
            max_workers=max_workers,
            thread_name_prefix=f"worker-{name}-"
        )
        
        self.active_tasks = {}
        self.lock = threading.Lock()
        
        # Statistics
        self.stats = {
            'submitted': 0,
            'completed': 0,
            'failed': 0,
            'cancelled': 0
        }
    
    def submit(self, fn, *args, **kwargs):
        """
        Submit a task to the worker pool.
        
        Args:
            fn: Function to execute
            *args, **kwargs: Arguments to pass to the function
            
        Returns:
            concurrent.futures.Future: Future representing the task
        """
        with self.lock:
            self.stats['submitted'] += 1
            
            # Submit task to executor
            future = self.executor.submit(fn, *args, **kwargs)
            
            # Add completion callback
            future.add_done_callback(self._task_done)
            
            # Store future
            task_id = id(future)
            self.active_tasks[task_id] = future
            
            return future
    
    def _task_done(self, future):
        """
        Callback when a task is done.
        
        Args:
            future: Completed future
        """
        with self.lock:
            # Update statistics
            if future.cancelled():
                self.stats['cancelled'] += 1
            elif future.exception() is not None:
                self.stats['failed'] += 1
            else:
                self.stats['completed'] += 1
            
            # Remove from active tasks
            task_id = id(future)
            if task_id in self.active_tasks:
                del self.active_tasks[task_id]
    
    def shutdown(self, wait=True):
        """
        Shutdown the worker pool.
        
        Args:
            wait (bool): Whether to wait for tasks to complete
        """
        self.executor.shutdown(wait=wait)
    
    def get_stats(self):
        """
        Get pool statistics.
        
        Returns:
            dict: Pool statistics
        """
        with self.lock:
            stats = self.stats.copy()
            stats['active'] = len(self.active_tasks)
            return stats
```

#### Resource Monitoring

```python
class ResourceMonitor:
    """Monitor system resources and provide throttling recommendations."""
    
    def __init__(self, check_interval=5.0, high_threshold=0.8, low_threshold=0.5):
        """
        Initialize resource monitor.
        
        Args:
            check_interval (float): Interval between resource checks in seconds
            high_threshold (float): High load threshold (0.0-1.0)
            low_threshold (float): Low load threshold (0.0-1.0)
        """
        self.check_interval = check_interval
        self.high_threshold = high_threshold
        self.low_threshold = low_threshold
        
        self.running = False
        self.thread = None
        
        # Resource usage history
        self.history = {
            'cpu': deque(maxlen=12),  # 1 minute at 5s interval
            'memory': deque(maxlen=12)
        }
        
        # Callbacks
        self.high_load_callbacks = []
        self.low_load_callbacks = []
        self.normal_load_callbacks = []
    
    def start(self):
        """Start resource monitoring."""
        if self.running:
            return
        
        self.running = True
        self.thread = threading.Thread(target=self._monitor_loop)
        self.thread.daemon = True
        self.thread.start()
    
    def stop(self):
        """Stop resource monitoring."""
        self.running = False
        if self.thread:
            self.thread.join(timeout=self.check_interval * 2)
    
    def _monitor_loop(self):
        """Main monitoring loop."""
        last_state = None
        
        while self.running:
            try:
                # Get current resource usage
                cpu_percent = psutil.cpu_percent(interval=1.0) / 100.0
                memory_percent = psutil.virtual_memory().percent / 100.0
                
                # Update history
                self.history['cpu'].append(cpu_percent)
                self.history['memory'].append(memory_percent)
                
                # Calculate average usage
                avg_cpu = sum(self.history['cpu']) / len(self.history['cpu'])
                avg_memory = sum(self.history['memory']) / len(self.history['memory'])
                
                # Determine current state
                if avg_cpu > self.high_threshold or avg_memory > self.high_threshold:
                    current_state = 'high'
                elif avg_cpu < self.low_threshold and avg_memory < self.low_threshold:
                    current_state = 'low'
                else:
                    current_state = 'normal'
                
                # Call appropriate callbacks if state changed
                if current_state != last_state:
                    if current_state == 'high':
                        for callback in self.high_load_callbacks:
                            callback(avg_cpu, avg_memory)
                    elif current_state == 'low':
                        for callback in self.low_load_callbacks:
                            callback(avg_cpu, avg_memory)
                    else:
                        for callback in self.normal_load_callbacks:
                            callback(avg_cpu, avg_memory)
                
                last_state = current_state
                
                # Sleep until next check
                time.sleep(self.check_interval)
            
            except Exception as e:
                logger.error(f"Error in resource monitor: {str(e)}")
                time.sleep(self.check_interval)
    
    def get_current_usage(self):
        """
        Get current resource usage.
        
        Returns:
            dict: Current resource usage
        """
        return {
            'cpu': psutil.cpu_percent() / 100.0,
            'memory': psutil.virtual_memory().percent / 100.0,
            'cpu_history': list(self.history['cpu']),
            'memory_history': list(self.history['memory'])
        }
    
    def on_high_load(self, callback):
        """
        Register callback for high load condition.
        
        Args:
            callback: Function to call with (cpu_usage, memory_usage) parameters
        """
        self.high_load_callbacks.append(callback)
    
    def on_low_load(self, callback):
        """
        Register callback for low load condition.
        
        Args:
            callback: Function to call with (cpu_usage, memory_usage) parameters
        """
        self.low_load_callbacks.append(callback)
    
    def on_normal_load(self, callback):
        """
        Register callback for normal load condition.
        
        Args:
            callback: Function to call with (cpu_usage, memory_usage) parameters
        """
        self.normal_load_callbacks.append(callback)
```

#### Backpressure Mechanisms

```python
class RateLimiter:
    """Rate limiter for controlling processing speed."""
    
    def __init__(self, max_rate=10, time_period=1.0):
        """
        Initialize rate limiter.
        
        Args:
            max_rate (int): Maximum number of operations per time period
            time_period (float): Time period in seconds
        """
        self.max_rate = max_rate
        self.time_period = time_period
        
        self.tokens = max_rate
        self.last_refill = time.time()
        self.lock = threading.Lock()
    
    def acquire(self, tokens=1, block=True, timeout=None):
        """
        Acquire tokens from the rate limiter.
        
        Args:
            tokens (int): Number of tokens to acquire
            block (bool): Whether to block until tokens are available
            timeout (float): Maximum time to wait in seconds
            
        Returns:
            bool: True if tokens were acquired, False otherwise
        """
        start_time = time.time()
        
        while True:
            with self.lock:
                # Refill tokens
                now = time.time()
                elapsed = now - self.last_refill
                
                if elapsed >= self.time_period:
                    # Full refill
                    self.tokens = self.max_rate
                    self.last_refill = now
                elif elapsed > 0:
                    # Partial refill
                    new_tokens = int(elapsed / self.time_period * self.max_rate)
                    if new_tokens > 0:
                        self.tokens = min(self.max_rate, self.tokens + new_tokens)
                        self.last_refill += new_tokens * self.time_period / self.max_rate
                
                # Check if we have enough tokens
                if self.tokens >= tokens:
                    self.tokens -= tokens
                    return True
            
            # If not blocking, return False
            if not block:
                return False
            
            # Check timeout
            if timeout is not None:
                if time.time() - start_time >= timeout:
                    return False
            
            # Sleep for a short time before trying again
            time.sleep(0.01)
    
    def set_rate(self, max_rate):
        """
        Set a new maximum rate.
        
        Args:
            max_rate (int): New maximum rate
        """
        with self.lock:
            self.max_rate = max_rate
```

## Implementation Timeline

### Week 1
- Implement enhanced job tracking with database storage
- Create worker pools for parallel processing
- Develop basic resource monitoring

### Week 2
- Implement job prioritization and queue management
- Add checkpointing for long-running jobs
- Develop rate limiting and backpressure mechanisms

### Week 3
- Implement job recovery from checkpoints
- Add resource-based adaptive throttling
- Integrate all components into cohesive system

### Week 4
- Implement job cancellation with proper resource cleanup
- Add comprehensive logging and reporting
- Develop distributed processing design (future implementation)

## Testing Strategy

1. **Unit Tests**
   - Test job state transitions
   - Verify worker pool task execution
   - Test rate limiting under controlled conditions

2. **Integration Tests**
   - Test end-to-end job submission and execution
   - Verify job recovery after simulated failures
   - Test resource monitoring and throttling

3. **Performance Tests**
   - Benchmark parallel processing with different worker counts
   - Test system under high load conditions
   - Measure throughput with and without rate limiting

4. **Stress Tests**
   - Test system with large numbers of concurrent jobs
   - Verify graceful degradation under resource constraints
   - Test recovery from simulated crashes

## Docker Considerations

All changes will follow the immutable container principle:

1. Develop and test changes locally
2. Update Dockerfile and requirements.txt as needed
3. Rebuild container to apply changes
4. Never modify running containers directly

## Success Criteria

1. Successfully process 100+ documents in parallel
2. Recover from failures without data loss
3. Maintain system stability under high load
4. Provide accurate job status and progress reporting
