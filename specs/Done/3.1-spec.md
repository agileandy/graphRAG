# Specification: Improve Document Processing (Phase 3.1)

## Overview

This specification outlines the improvements needed for document processing in the GraphRAG system, focusing on fixing ChromaDB issues, enhancing deduplication, and expanding concept extraction capabilities.

## 1. Fix ChromaDB Issues

### Problem Statement

The current implementation encounters errors when processing large books, specifically "Error in compaction: Failed to apply logs to the metadata segment". Only 8 out of 11 books from the Industry5 folder were successfully processed.

### Requirements

1. **ChromaDB Version Update**
   - Update ChromaDB to the latest stable version
   - Test compatibility with existing data
   - Document migration process if needed

2. **Error Handling Improvements**
   - Implement robust error handling for ChromaDB operations
   - Add detailed logging for ChromaDB errors
   - Create recovery mechanisms for failed operations

3. **Retry Logic**
   - Implement exponential backoff retry for failed document additions
   - Set configurable retry limits and delays
   - Log retry attempts and outcomes

4. **Chunk Size Optimization**
   - Implement adaptive chunk sizing based on document characteristics
   - Add configuration options for chunk size parameters
   - Benchmark different chunk sizes for performance and reliability

### Implementation Details

#### ChromaDB Update

```python
# Update requirements.txt
# chromadb==0.4.18  # Replace with latest version

# Add version check in code
def check_chromadb_version():
    import chromadb
    current_version = chromadb.__version__
    min_required = "0.4.18"  # Update to latest tested version
    
    if parse_version(current_version) < parse_version(min_required):
        logger.warning(f"ChromaDB version {current_version} is older than recommended {min_required}")
```

#### Error Handling

```python
def add_document_to_chromadb(document, retries=3):
    """Add document to ChromaDB with retry logic."""
    retry_count = 0
    backoff = 1  # Initial backoff in seconds
    
    while retry_count <= retries:
        try:
            # Existing document addition code
            collection.add(
                documents=document.chunks,
                metadatas=document.metadatas,
                ids=document.ids
            )
            logger.info(f"Successfully added document: {document.title}")
            return True
        except Exception as e:
            retry_count += 1
            if retry_count <= retries:
                logger.warning(f"Error adding document to ChromaDB: {e}. Retry {retry_count}/{retries} in {backoff}s")
                time.sleep(backoff)
                backoff *= 2  # Exponential backoff
            else:
                logger.error(f"Failed to add document after {retries} retries: {e}")
                return False
```

#### Chunk Size Optimization

```python
def optimize_chunk_size(document_text, default_size=1000):
    """Determine optimal chunk size based on document characteristics."""
    # Simple heuristic: adjust chunk size based on document length
    doc_length = len(document_text)
    
    if doc_length > 1000000:  # Very large document (>1M chars)
        return 500  # Smaller chunks for very large documents
    elif doc_length > 500000:  # Large document
        return 750
    else:
        return default_size
```

## 2. Enhance Deduplication

### Problem Statement

Multiple entries for the same books appear in the database, indicating insufficient deduplication logic.

### Requirements

1. **Content-based Deduplication**
   - Implement document fingerprinting using content hashing
   - Create unique identifiers based on document content
   - Check for duplicates before adding new documents

2. **Metadata-based Deduplication**
   - Use metadata (title, author, file path) for initial duplicate detection
   - Implement fuzzy matching for titles to catch slight variations
   - Create normalized forms of metadata for comparison

3. **Cleanup Tools**
   - Develop tools to identify and remove duplicate entries
   - Create reports of duplicate documents
   - Implement safe removal that preserves relationships

4. **Duplicate Logging**
   - Add detailed logging for duplicate detection
   - Track frequency and patterns of duplicates
   - Generate statistics on duplication rates

### Implementation Details

#### Content Hashing

```python
def generate_document_hash(document_text):
    """Generate a hash based on document content."""
    import hashlib
    
    # Normalize text (remove whitespace, convert to lowercase)
    normalized_text = re.sub(r'\s+', ' ', document_text).lower().strip()
    
    # Generate SHA-256 hash
    return hashlib.sha256(normalized_text.encode('utf-8')).hexdigest()
```

#### Duplicate Detection

```python
def is_duplicate_document(new_doc, collection):
    """Check if document already exists in the collection."""
    # Check by hash
    doc_hash = generate_document_hash(new_doc.text)
    
    # Query for existing documents with same hash
    results = collection.query(
        where={"hash": doc_hash},
        limit=1
    )
    
    if results and len(results['documents']) > 0:
        logger.info(f"Duplicate detected: {new_doc.title} matches existing document")
        return True
    
    # Check by title similarity if no exact hash match
    title_results = collection.query(
        where={"title": {"$contains": new_doc.title}},
        limit=5
    )
    
    if title_results and len(title_results['documents']) > 0:
        # Implement fuzzy matching on titles
        for doc in title_results['documents']:
            similarity = calculate_title_similarity(new_doc.title, doc['title'])
            if similarity > 0.9:  # 90% similarity threshold
                logger.info(f"Potential duplicate: {new_doc.title} similar to {doc['title']}")
                return True
    
    return False
```

#### Cleanup Tool

```python
def find_and_report_duplicates():
    """Find and report duplicate documents in the database."""
    # Get all documents
    all_docs = collection.get()
    
    # Group by hash
    docs_by_hash = {}
    for i, doc_hash in enumerate(all_docs['metadatas']['hash']):
        if doc_hash not in docs_by_hash:
            docs_by_hash[doc_hash] = []
        docs_by_hash[doc_hash].append({
            'id': all_docs['ids'][i],
            'title': all_docs['metadatas']['title'][i]
        })
    
    # Find duplicates
    duplicates = {h: docs for h, docs in docs_by_hash.items() if len(docs) > 1}
    
    # Generate report
    report = {
        'total_documents': len(all_docs['ids']),
        'unique_documents': len(docs_by_hash),
        'duplicate_sets': len(duplicates),
        'duplicates': duplicates
    }
    
    return report
```

## 3. Expand Concept Extraction

### Problem Statement

Domain-specific concepts (like "Industry 4.0" and "Industry 5.0") are not being extracted, and the current concept list is heavily biased toward AI/ML concepts.

### Requirements

1. **Dynamic Concept Extraction**
   - Implement NLP-based concept extraction
   - Support domain-specific concept identification
   - Create adaptive thresholds for concept significance

2. **LLM Integration**
   - Integrate with LLMs for sophisticated concept extraction
   - Implement prompt engineering for optimal concept identification
   - Balance precision and recall in concept extraction

3. **Concept Weighting**
   - Add weights to concepts based on frequency and relevance
   - Implement TF-IDF or similar algorithms for concept significance
   - Create normalized weights across different documents

### Implementation Details

#### NLP-based Concept Extraction

```python
def extract_concepts_nlp(document_text):
    """Extract concepts using NLP techniques."""
    import spacy
    
    # Load SpaCy model
    nlp = spacy.load("en_core_web_lg")
    
    # Process text
    doc = nlp(document_text)
    
    # Extract noun phrases as potential concepts
    noun_phrases = [chunk.text for chunk in doc.noun_chunks]
    
    # Extract named entities
    entities = [ent.text for ent in doc.ents]
    
    # Combine and deduplicate
    all_concepts = list(set(noun_phrases + entities))
    
    # Filter by length and frequency
    filtered_concepts = [
        concept for concept in all_concepts
        if len(concept.split()) <= 4 and document_text.count(concept) >= 3
    ]
    
    return filtered_concepts
```

#### LLM-based Concept Extraction

```python
def extract_concepts_llm(document_text, max_concepts=20):
    """Extract concepts using LLM."""
    from langchain.llms import OpenAI
    
    # Create prompt
    prompt = f"""
    Extract the key concepts from the following text. 
    Focus on domain-specific terminology, technical terms, and important themes.
    Return only a JSON array of strings with no explanation.
    
    Text: {document_text[:2000]}...
    
    Concepts:
    """
    
    # Call LLM
    llm = OpenAI(temperature=0.1)
    response = llm(prompt)
    
    # Parse response
    try:
        import json
        concepts = json.loads(response)
        return concepts[:max_concepts]
    except:
        logger.error(f"Failed to parse LLM response: {response}")
        return []
```

#### Concept Weighting

```python
def calculate_concept_weights(concepts, document_text, all_documents):
    """Calculate weights for concepts using TF-IDF."""
    from sklearn.feature_extraction.text import TfidfVectorizer
    
    # Create corpus of all documents
    corpus = [doc.text for doc in all_documents] + [document_text]
    
    # Calculate TF-IDF
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(corpus)
    feature_names = vectorizer.get_feature_names_out()
    
    # Get weights for current document (last in the corpus)
    doc_index = len(corpus) - 1
    feature_index = tfidf_matrix[doc_index].nonzero()[1]
    tfidf_scores = zip(feature_index, [tfidf_matrix[doc_index, x] for x in feature_index])
    
    # Create weight dictionary
    weights = {}
    for concept in concepts:
        # Find closest matching feature
        best_match = None
        best_score = 0
        
        for idx, score in tfidf_scores:
            feature = feature_names[idx]
            if feature in concept.lower() or concept.lower() in feature:
                if score > best_score:
                    best_match = feature
                    best_score = score
        
        weights[concept] = best_score if best_match else 0.1
    
    return weights
```

## Implementation Timeline

### Week 1
- Update ChromaDB to latest version
- Implement basic retry logic
- Develop document hashing for deduplication

### Week 2
- Implement adaptive chunk sizing
- Create duplicate detection and reporting tools
- Integrate NLP-based concept extraction

## Testing Strategy

1. **Unit Tests**
   - Test hash generation with various document types
   - Verify retry logic with simulated failures
   - Test concept extraction with sample documents

2. **Integration Tests**
   - Test end-to-end document processing with large documents
   - Verify deduplication across multiple document additions
   - Test concept extraction and weighting in full pipeline

3. **Performance Tests**
   - Benchmark processing times with different chunk sizes
   - Measure memory usage during large document processing
   - Test system with deliberately large document sets

## Docker Considerations

All changes will follow the immutable container principle:

1. Develop and test changes locally
2. Update Dockerfile and requirements.txt as needed
3. Rebuild container to apply changes
4. Never modify running containers directly

## Success Criteria

1. Successfully process all 11 books from the Industry5 folder
2. Zero duplicate entries when adding the same document multiple times
3. Identify domain-specific concepts like "Industry 4.0" and "Industry 5.0"
4. Reduce ChromaDB-related errors by 90%

## 4. Local Deployment Outside Container

### Problem Statement

The current system is designed to run within a Docker container, but there's a need to run all services locally on macOS for development and testing purposes without containerization.

### Requirements

1. **Service Wrappers**
   - Create daemon wrappers for Neo4j, API server, and MPC server
   - Implement proper startup/shutdown scripts
   - Ensure services can run in the background

2. **Process Management**
   - Implement start/stop/restart commands for each service
   - Create monitoring scripts to check service health
   - Add logging for service status changes

3. **Configuration Management**
   - Create configuration files for local deployment
   - Implement environment variable handling
   - Support different profiles (dev, test, prod)

4. **Resource Management**
   - Implement resource monitoring for local services
   - Add automatic restart for crashed services
   - Create resource usage reporting

### Implementation Details

#### Service Wrappers

```bash
#!/bin/bash
# graphrag-service.sh - Service management script for GraphRAG

# Configuration
CONFIG_FILE="$HOME/.graphrag/config.env"
LOG_DIR="$HOME/.graphrag/logs"
PID_DIR="$HOME/.graphrag/pids"

# Load configuration
if [ -f "$CONFIG_FILE" ]; then
    source "$CONFIG_FILE"
else
    echo "Configuration file not found. Creating default configuration..."
    mkdir -p "$(dirname "$CONFIG_FILE")"
    cat > "$CONFIG_FILE" << EOF
# GraphRAG Configuration
NEO4J_HOME=$HOME/.graphrag/neo4j
NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=graphrag
CHROMA_PERSIST_DIRECTORY=$HOME/.graphrag/data/chromadb
GRAPHRAG_API_PORT=5000
GRAPHRAG_MPC_PORT=8765
GRAPHRAG_LOG_LEVEL=INFO
EOF
    source "$CONFIG_FILE"
fi

# Create necessary directories
mkdir -p "$LOG_DIR" "$PID_DIR"

# Function to start Neo4j
start_neo4j() {
    echo "Starting Neo4j..."
    if [ -f "$PID_DIR/neo4j.pid" ]; then
        echo "Neo4j is already running."
        return 1
    fi
    
    "$NEO4J_HOME/bin/neo4j" start
    
    # Wait for Neo4j to start
    echo "Waiting for Neo4j to start..."
    for i in {1..30}; do
        if curl -s http://localhost:7474 > /dev/null; then
            echo "Neo4j started successfully."
            return 0
        fi
        sleep 1
    done
    
    echo "Failed to start Neo4j."
    return 1
}

# Function to start API server
start_api() {
    echo "Starting API server..."
    if [ -f "$PID_DIR/api.pid" ]; then
        echo "API server is already running."
        return 1
    fi
    
    cd "$(dirname "$0")/.." && \
    gunicorn \
        --bind 0.0.0.0:$GRAPHRAG_API_PORT \
        --workers 2 \
        --threads 4 \
        --timeout 120 \
        --access-logfile "$LOG_DIR/gunicorn-access.log" \
        --error-logfile "$LOG_DIR/gunicorn-error.log" \
        --log-level $GRAPHRAG_LOG_LEVEL \
        --daemon \
        --pid "$PID_DIR/api.pid" \
        src.api.wsgi:app
    
    # Wait for API server to start
    echo "Waiting for API server to start..."
    for i in {1..15}; do
        if curl -s http://localhost:$GRAPHRAG_API_PORT/health > /dev/null; then
            echo "API server started successfully."
            return 0
        fi
        sleep 1
    done
    
    echo "Failed to start API server."
    return 1
}

# Function to start MPC server
start_mpc() {
    echo "Starting MPC server..."
    if [ -f "$PID_DIR/mpc.pid" ]; then
        echo "MPC server is already running."
        return 1
    fi
    
    cd "$(dirname "$0")/.." && \
    python -m src.mpc.server --host 0.0.0.0 --port $GRAPHRAG_MPC_PORT > "$LOG_DIR/mpc.log" 2>&1 &
    echo $! > "$PID_DIR/mpc.pid"
    
    # Wait for MPC server to start
    echo "Waiting for MPC server to start..."
    sleep 3
    
    if [ -f "$PID_DIR/mpc.pid" ]; then
        PID=$(cat "$PID_DIR/mpc.pid")
        if ps -p $PID > /dev/null; then
            echo "MPC server started successfully."
            return 0
        fi
    fi
    
    echo "Failed to start MPC server."
    return 1
}

# Function to stop Neo4j
stop_neo4j() {
    echo "Stopping Neo4j..."
    "$NEO4J_HOME/bin/neo4j" stop
    rm -f "$PID_DIR/neo4j.pid"
    echo "Neo4j stopped."
}

# Function to stop API server
stop_api() {
    echo "Stopping API server..."
    if [ -f "$PID_DIR/api.pid" ]; then
        PID=$(cat "$PID_DIR/api.pid")
        kill $PID
        rm -f "$PID_DIR/api.pid"
        echo "API server stopped."
    else
        echo "API server is not running."
    fi
}

# Function to stop MPC server
stop_mpc() {
    echo "Stopping MPC server..."
    if [ -f "$PID_DIR/mpc.pid" ]; then
        PID=$(cat "$PID_DIR/mpc.pid")
        kill $PID
        rm -f "$PID_DIR/mpc.pid"
        echo "MPC server stopped."
    else
        echo "MPC server is not running."
    fi
}

# Function to check status
status() {
    echo "GraphRAG Service Status:"
    
    # Check Neo4j
    if curl -s http://localhost:7474 > /dev/null; then
        echo "Neo4j: Running"
    else
        echo "Neo4j: Stopped"
    fi
    
    # Check API server
    if [ -f "$PID_DIR/api.pid" ]; then
        PID=$(cat "$PID_DIR/api.pid")
        if ps -p $PID > /dev/null; then
            echo "API Server: Running (PID: $PID)"
        else
            echo "API Server: Crashed (PID file exists but process is not running)"
            rm -f "$PID_DIR/api.pid"
        fi
    else
        echo "API Server: Stopped"
    fi
    
    # Check MPC server
    if [ -f "$PID_DIR/mpc.pid" ]; then
        PID=$(cat "$PID_DIR/mpc.pid")
        if ps -p $PID > /dev/null; then
            echo "MPC Server: Running (PID: $PID)"
        else
            echo "MPC Server: Crashed (PID file exists but process is not running)"
            rm -f "$PID_DIR/mpc.pid"
        fi
    else
        echo "MPC Server: Stopped"
    fi
}

# Main command handling
case "$1" in
    start)
        start_neo4j
        start_api
        start_mpc
        ;;
    stop)
        stop_mpc
        stop_api
        stop_neo4j
        ;;
    restart)
        stop_mpc
        stop_api
        stop_neo4j
        sleep 2
        start_neo4j
        start_api
        start_mpc
        ;;
    status)
        status
        ;;
    start-neo4j)
        start_neo4j
        ;;
    start-api)
        start_api
        ;;
    start-mpc)
        start_mpc
        ;;
    stop-neo4j)
        stop_neo4j
        ;;
    stop-api)
        stop_api
        ;;
    stop-mpc)
        stop_mpc
        ;;
    *)
        echo "Usage: $0 {start|stop|restart|status|start-neo4j|start-api|start-mpc|stop-neo4j|stop-api|stop-mpc}"
        exit 1
        ;;
esac

exit 0
```

#### Process Monitoring Script

```python
#!/usr/bin/env python3
"""
GraphRAG Service Monitor

This script monitors the GraphRAG services and restarts them if they crash.
It can be run as a cron job or as a daemon.
"""
import os
import sys
import time
import signal
import logging
import subprocess
import psutil
from datetime import datetime

# Configuration
LOG_FILE = os.path.expanduser("~/.graphrag/logs/monitor.log")
PID_DIR = os.path.expanduser("~/.graphrag/pids")
CONFIG_FILE = os.path.expanduser("~/.graphrag/config.env")
CHECK_INTERVAL = 60  # seconds

# Configure logging
logging.basicConfig(
    filename=LOG_FILE,
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# Load configuration
def load_config():
    """Load configuration from environment file."""
    config = {}
    if os.path.exists(CONFIG_FILE):
        with open(CONFIG_FILE, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    key, value = line.split('=', 1)
                    config[key] = value
    return config

config = load_config()

# Service definitions
SERVICES = [
    {
        'name': 'neo4j',
        'pid_file': os.path.join(PID_DIR, 'neo4j.pid'),
        'check_url': 'http://localhost:7474',
        'start_cmd': os.path.join(os.path.dirname(os.path.abspath(__file__)), 'graphrag-service.sh') + ' start-neo4j'
    },
    {
        'name': 'api',
        'pid_file': os.path.join(PID_DIR, 'api.pid'),
        'check_url': f"http://localhost:{config.get('GRAPHRAG_API_PORT', '5000')}/health",
        'start_cmd': os.path.join(os.path.dirname(os.path.abspath(__file__)), 'graphrag-service.sh') + ' start-api'
    },
    {
        'name': 'mpc',
        'pid_file': os.path.join(PID_DIR, 'mpc.pid'),
        'start_cmd': os.path.join(os.path.dirname(os.path.abspath(__file__)), 'graphrag-service.sh') + ' start-mpc'
    }
]

def check_service(service):
    """Check if a service is running and restart it if necessary."""
    name = service['name']
    pid_file = service['pid_file']
    
    # Check if PID file exists
    if not os.path.exists(pid_file):
        logging.warning(f"{name} service is not running (no PID file)")
        return False
    
    # Read PID from file
    try:
        with open(pid_file, 'r') as f:
            pid = int(f.read().strip())
    except (IOError, ValueError) as e:
        logging.error(f"Error reading PID file for {name}: {e}")
        return False
    
    # Check if process is running
    try:
        process = psutil.Process(pid)
        if process.is_running():
            # Additional check for HTTP services
            if 'check_url' in service:
                try:
                    result = subprocess.run(
                        ['curl', '-s', '-o', '/dev/null', '-w', '%{http_code}', service['check_url']],
                        capture_output=True,
                        text=True,
                        timeout=5
                    )
                    if result.stdout.strip() != '200':
                        logging.warning(f"{name} service is not responding properly (HTTP {result.stdout.strip()})")
                        return False
                except subprocess.SubprocessError as e:
                    logging.warning(f"Error checking {name} service URL: {e}")
                    return False
            
            # Service is running
            return True
        else:
            logging.warning(f"{name} service is not running (PID {pid} not found)")
            return False
    except psutil.NoSuchProcess:
        logging.warning(f"{name} service is not running (PID {pid} not found)")
        return False

def restart_service(service):
    """Restart a service."""
    name = service['name']
    pid_file = service['pid_file']
    start_cmd = service['start_cmd']
    
    logging.info(f"Restarting {name} service...")
    
    # Remove PID file if it exists
    if os.path.exists(pid_file):
        os.remove(pid_file)
    
    # Start service
    try:
        result = subprocess.run(
            start_cmd,
            shell=True,
            capture_output=True,
            text=True,
            timeout=30
        )
        if result.returncode == 0:
            logging.info(f"Successfully restarted {name} service")
            return True
        else:
            logging.error(f"Failed to restart {name} service: {result.stderr}")
            return False
    except subprocess.SubprocessError as e:
        logging.error(f"Error restarting {name} service: {e}")
        return False

def monitor_services():
    """Monitor all services and restart them if necessary."""
    for service in SERVICES:
        if not check_service(service):
            restart_service(service)

def collect_resource_usage():
    """Collect resource usage information."""
    cpu_percent = psutil.cpu_percent(interval=1)
    memory = psutil.virtual_memory()
    disk = psutil.disk_usage('/')
    
    logging.info(f"Resource Usage - CPU: {cpu_percent}%, Memory: {memory.percent}%, Disk: {disk.percent}%")

def main():
    """Main function."""
    logging.info("Starting GraphRAG Service Monitor")
    
    try:
        while True:
            monitor_services()
            collect_resource_usage()
            time.sleep(CHECK_INTERVAL)
    except KeyboardInterrupt:
        logging.info("Stopping GraphRAG Service Monitor")
        sys.exit(0)

if __name__ == "__main__":
    main()
```

### Implementation Timeline

#### Week 1
- Create basic service wrapper scripts
- Implement start/stop/restart commands
- Test basic functionality

#### Week 2
- Implement monitoring script
- Add resource usage reporting
- Create configuration management

#### Week 3
- Test all components together
- Create documentation
- Package for easy installation

## Testing Strategy

1. **Unit Tests**
   - Test service wrapper functions
   - Verify configuration loading
   - Test monitoring functionality

2. **Integration Tests**
   - Test complete startup/shutdown sequence
   - Verify service recovery after crashes
   - Test resource monitoring and reporting

3. **Performance Tests**
   - Compare performance with containerized version
   - Measure resource usage
   - Test with different configuration profiles

## Success Criteria

1. All services can be started, stopped, and monitored outside of Docker
2. Services automatically recover from crashes
3. Resource usage is properly monitored and reported
4. Performance is comparable to the containerized version
