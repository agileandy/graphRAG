# Specification: Embedding Model Update

## 1. Overview

This specification outlines the changes required to update the embedding model configuration in the GraphRAG system. The current implementation incorrectly references OpenAI's text-embedding-ada-002 model, while the actual implementation uses local models. This specification aims to standardize on using Ollama with the "snowflake-arctic-embed2:latest" model for embeddings and to add support for optional reranking with "qllama/bge-reranker-large:latest".

## 2. Background

The GraphRAG system currently has inconsistencies between the documented embedding approach and the actual implementation. The design document mentions using OpenAI's text-embedding-ada-002 model, but the system is actually configured to use local models through LM Studio. This specification aims to clarify and standardize the embedding approach by using Ollama with specific models optimized for embedding and reranking tasks.

## 3. Requirements

### 3.1 Functional Requirements

1. **Default Embedding Model**: Update the system to use Ollama with "snowflake-arctic-embed2:latest" as the default embedding model.
2. **Reranking Support**: Add support for optional reranking using Ollama with "qllama/bge-reranker-large:latest".
3. **Model Separation**: Ensure that Phi-4 is used only for concept extraction and NLP processing, not for embeddings.
4. **Configuration Flexibility**: Allow easy switching between embedding models through configuration.
5. **Fallback Mechanism**: Maintain a fallback mechanism if the primary embedding model is unavailable.

### 3.2 Non-Functional Requirements

1. **Performance**: The embedding generation should maintain or improve current performance metrics.
2. **Resource Efficiency**: The embedding process should be optimized for resource usage.
3. **Compatibility**: The changes should be backward compatible with existing document collections.
4. **Documentation**: Update all relevant documentation to reflect the new embedding approach.

## 4. Design

### 4.1 Configuration Updates

Update the `llm_config.json` file to specify the embedding models:

```json
{
    "primary_provider": {
        "type": "openai-compatible",
        "api_base": "http://192.168.1.21:1234/v1",
        "api_key": "dummy-key",
        "model": "lmstudio-community/Phi-4-mini-reasoning-MLX-4bit",
        "temperature": 0.1,
        "max_tokens": 1000,
        "timeout": 60
    },
    "fallback_provider": {
        "type": "ollama",
        "api_base": "http://localhost:11434",
        "model": "llama2",
        "temperature": 0.1,
        "max_tokens": 1000,
        "timeout": 60
    },
    "embedding_provider": {
        "type": "ollama",
        "api_base": "http://localhost:11434",
        "model": "snowflake-arctic-embed2:latest",
        "timeout": 60
    },
    "reranker_provider": {
        "type": "ollama",
        "api_base": "http://localhost:11434",
        "model": "qllama/bge-reranker-large:latest",
        "timeout": 60
    }
}
```

### 4.2 Code Changes

#### 4.2.1 LLM Provider Updates

Update the `src/llm/llm_provider.py` file to:
1. Add a dedicated reranker provider class or method
2. Update the factory function to create providers based on the updated configuration
3. Ensure the embedding dimension handling is compatible with the 1024-dimensional embeddings from Arctic Embed

#### 4.2.2 Vector Database Updates

Update the `src/database/vector_db.py` file to:
1. Ensure compatibility with 1024-dimensional embeddings
2. Add support for reranking in the query method (optional)

#### 4.2.3 Document Processor Updates

Update the `src/processing/document_processor.py` file to:
1. Use the correct embedding provider for generating embeddings
2. Ensure batch processing is optimized for the new embedding model

### 4.3 Reranking Implementation

Add a new reranking module in `src/search/reranker.py`:

```python
"""
Reranker module for improving search results.
"""
import logging
from typing import List, Dict, Any, Optional

from src.llm.llm_provider import LLMProvider, create_llm_provider

logger = logging.getLogger(__name__)

class Reranker:
    """
    Reranker for improving search results using a cross-encoder model.
    """
    def __init__(self, llm_provider: Optional[LLMProvider] = None, config: Optional[Dict[str, Any]] = None):
        """
        Initialize reranker.
        
        Args:
            llm_provider: LLM provider for reranking
            config: Configuration dictionary
        """
        if llm_provider:
            self.provider = llm_provider
        elif config:
            self.provider = create_llm_provider(config)
        else:
            # Load default configuration
            from src.utils.config import load_config
            reranker_config = load_config().get("reranker_provider", {})
            self.provider = create_llm_provider(reranker_config)
    
    def rerank(self, query: str, documents: List[Dict[str, Any]], top_k: int = 5) -> List[Dict[str, Any]]:
        """
        Rerank documents based on relevance to query.
        
        Args:
            query: Query text
            documents: List of documents with text and metadata
            top_k: Number of top results to return
            
        Returns:
            Reranked documents
        """
        # Implementation details will depend on the specific reranker model
        # This is a placeholder for the actual implementation
        pass
```

## 5. Implementation Plan

1. **Update Configuration**: Update the `llm_config.json` file with the new embedding and reranking configuration.
2. **Update LLM Provider**: Modify the LLM provider to support the new embedding model and reranking.
3. **Add Reranker Module**: Implement the reranker module for optional result reranking.
4. **Update Vector Database**: Ensure compatibility with the new embedding dimensions.
5. **Update Document Processor**: Modify the document processor to use the correct embedding provider.
6. **Testing**: Test the changes with various document types and queries.
7. **Documentation**: Update all relevant documentation to reflect the new embedding approach.

## 6. Testing

1. **Unit Tests**: Update and add unit tests for the modified components.
2. **Integration Tests**: Test the end-to-end document processing and search functionality.
3. **Performance Tests**: Compare the performance of the new embedding model with the previous approach.
4. **Compatibility Tests**: Ensure compatibility with existing document collections.

## 7. Acceptance Criteria

1. The system successfully uses Ollama with "snowflake-arctic-embed2:latest" for embeddings.
2. Optional reranking with "qllama/bge-reranker-large:latest" is available and functional.
3. Phi-4 is used only for concept extraction and NLP processing, not for embeddings.
4. The system maintains or improves current performance metrics.
5. All documentation is updated to reflect the new embedding approach.

## 8. References

- [Snowflake Arctic Embed Documentation](https://huggingface.co/Snowflake/arctic-embed-2)
- [BGE Reranker Documentation](https://huggingface.co/BAAI/bge-reranker-large)
- [Ollama Documentation](https://ollama.ai/library)
