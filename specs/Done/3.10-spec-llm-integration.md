# 3.10 Specification: LLM Integration Consolidation

## Bug Description

The GraphRAG system currently has two separate implementations for LLM-based concept extraction:

1. A newer implementation in `src/llm/` that properly supports both local models through LM Studio (via OpenAI-compatible API) and actual OpenAI models. This implementation uses the configuration from `config/llm_config.json` which correctly points to the local Phi-4 model.

2. An older implementation in `src/processing/concept_extractor.py` that directly uses the OpenAI client and looks for the `OPENAI_API_KEY` environment variable.

When adding documents, the system is using the older implementation and displaying the warning: "OpenAI API key not found. LLM-based concept extraction will not be available." This prevents the system from using the local Phi-4 model through LM Studio as intended.

## Requirements

1. Consolidate the LLM integration code to use only the newer implementation
2. Ensure all document processing uses the local Phi-4 model through LM Studio
3. Remove dependencies on the OpenAI API key for basic functionality
4. Deprecate and eventually remove the old implementation

## Implementation Details

1. **Code Refactoring**:
   - Update `src/processing/concept_extractor.py` to use the `LLMManager` from `src/llm/llm_provider.py`
   - Remove direct OpenAI client usage from `concept_extractor.py`
   - Ensure all concept extraction uses the configuration from `config/llm_config.json`
   - Add deprecation warnings to any functions in the old implementation that are still needed temporarily

2. **Configuration Updates**:
   - Ensure the LLM configuration is properly loaded during initialization
   - Add fallback to rule-based extraction if LLM connection fails
   - Add clear error messages for configuration issues

3. **Testing**:
   - Test concept extraction with the local Phi-4 model
   - Verify that the system works without an OpenAI API key
   - Test fallback to rule-based extraction when LLM is unavailable

## Acceptance Criteria

1. The system uses the local Phi-4 model through LM Studio for concept extraction
2. No warnings about missing OpenAI API key are displayed during normal operation
3. The system falls back to rule-based extraction if LLM connection fails
4. All document processing uses the same LLM integration code
5. Old implementation is properly deprecated with warnings

## Notes

- This change will improve the user experience by removing unnecessary warnings
- It will also make the system more self-contained by reducing dependencies on external APIs
- Consider adding a configuration option to disable LLM-based extraction entirely for users who prefer rule-based extraction
- The advanced PDF processing capabilities in `src/loaders/pdf_loader.py` should be maintained and are not affected by this change