# graphRAG Design Document

## Objectives

GraphRAG is an advanced information retrieval and generation system designed to enhance Large Language Models (LLMs) by leveraging the power of knowledge graphs alongside traditional vector-based retrieval. Its primary goal is to overcome the limitations of standard Retrieval-Augmented Generation (RAG) systems, which primarily rely on semantic similarity and may miss nuanced relationships and contextual understanding inherent in structured knowledge. By integrating graph databases with vector databases, GraphRAG aims to provide more accurate, context-aware, and comprehensive responses.

The core objectives are:
- **Improve Answer Quality**: Enhance the relevance, accuracy, and depth of information retrieved and generated by LLMs.
- **Enable Deeper Understanding**: Allow the system to understand and utilize explicit relationships between entities and concepts within a corpus of documents.
- **Facilitate Complex Queries**: Support queries that require reasoning over interconnected information, not just keyword or semantic matching.
- **Scalable Knowledge Management**: Provide a robust framework for ingesting, processing, storing, and querying large volumes of documents and the knowledge extracted from them.
- **Modular and Extensible Architecture**: Design a system that can be easily extended with new data loaders, processing modules, and query capabilities.

Key features supporting these objectives include:
- **Hybrid Search**: Combine vector similarity with graph traversal to yield more contextually relevant search results.
- **Concept Extraction**: Identify key concepts and their relationships from documents.
- **Asynchronous Processing**: Handle large batch operations efficiently without blocking system resources.
- **AI Agent Integration**: Provide tools and interfaces for AI agents to interact with and utilize the system's capabilities.
- **Duplicate Detection**: Prevent the addition of duplicate documents into the system to maintain data integrity.

## Principles

The design and development of GraphRAG are guided by the following principles:

- **Accuracy and Relevance**: Prioritize the delivery of precise and contextually appropriate information.
- **Depth of Understanding**: Strive to go beyond surface-level information retrieval by capturing and utilizing complex relationships.
- **Context-Awareness**: Ensure that responses and insights are grounded in the relevant context derived from the knowledge graph and source documents.
- **Scalability**: Design the system to efficiently handle growing volumes of data and an increasing number of users and queries.
- **Modularity and Extensibility**: Build components that are independent and can be easily updated, replaced, or extended to accommodate new functionalities and technologies.
- **Reliability and Robustness**: Ensure consistent system uptime and effective error handling to provide a dependable service.
- **Maintainability**: Develop well-structured, documented code and clear architectural patterns to facilitate ongoing maintenance and evolution.
- **Security**: Implement appropriate measures to protect data integrity and confidentiality, especially when handling sensitive information.

## Approach

The core approach of GraphRAG is to build and utilize a knowledge graph in conjunction with vector embeddings. This involves:

1.  **Document Processing**:
    *   Ingesting documents in various formats.
    *   Chunking documents into manageable, semantically coherent pieces.
    *   Generating vector embeddings for these chunks.
    *   Extracting key concepts and identifying relationships between them using NLP and LLM techniques.
    *   Storing document chunks and embeddings in a vector database (ChromaDB) and concepts/relationships in a graph database (Neo4j).

2.  **Hybrid Retrieval**:
    *   When a query is received, perform a hybrid search that leverages both semantic similarity (from vector embeddings) and contextual relationships (from the knowledge graph).
    *   This allows for traversing related concepts and information, providing more comprehensive and nuanced answers than vector search alone.

3.  **System Architecture**:
    *   **Database Layer**: Utilizes Neo4j for the knowledge graph and ChromaDB for vector embeddings.
    *   **Processing Layer**: Handles document ingestion, chunking, embedding, concept extraction, and relationship building. Includes job management for asynchronous tasks.
    *   **API Layer**: Provides endpoints for search, document ingestion, concept retrieval, and job status.
    *   **MCP (Mode Control Protocol) Server**: Facilitates interaction with AI agents.

This approach enables the system to understand not just the content of documents but also the intricate connections between different pieces of information.

## Optimisation

This section covers general application and performance optimisation strategies. Database-specific optimisations are detailed in the "Database Design and Optimisation" section.

**General Performance Strategies:**

*   **Vector Search Scaling:** For very large document collections (e.g., >100,000 documents), consider sharding the vector database (ChromaDB) to distribute the load and improve query times.
*   **Neo4j Indexing:** Ensure that indexes are created on frequently queried properties in the Neo4j graph database. This is crucial for efficient graph traversals and lookups.
    ```cypher
    // Example indexes (more detailed list in Database Optimisation section)
    CREATE INDEX ON :Concept(name);
    CREATE INDEX ON :Document(title);
    ```
*   **Batch Processing:** When adding documents or performing other bulk operations, use appropriate batch sizes (e.g., 100-1000 items per batch) to balance throughput and resource consumption. This applies to both vector database additions and graph database transactions.
*   **Memory Management:**
    *   Monitor the memory usage of all components, especially Neo4j.
    *   Adjust JVM heap and page cache settings for Neo4j as needed, based on the size of the graph and available system resources.
*   **Asynchronous Processing:** Leverage asynchronous task processing for long-running operations like document ingestion and embedding generation to prevent blocking the main application threads and improve responsiveness. (This is also mentioned under Objectives and Approach).
*   **Efficient Chunking:** The document chunking strategy (semantic, size-constrained, structure-aware) is itself an optimisation to ensure that embeddings are meaningful and search results are relevant.
*   **LLM Calls:** Optimise interactions with Large Language Models:
    *   Batch requests where possible.
    *   Implement caching for LLM responses to avoid redundant calls for identical inputs.
    *   Use efficient and concise prompts.
    *   Select appropriate models for specific tasks (e.g., smaller/faster models for less complex tasks).

## Configuration

This section covers general application configuration, with a focus on port management.

**Centralized Port Configuration:**

GraphRAG employs a centralized system for managing port assignments, primarily defined in [`src/config/ports.py`](src/config/ports.py). This system:
*   Defines default port values for all services.
*   Allows overriding port values via environment variables (prefixed with `GRAPHRAG_PORT_`). These are typically loaded from an `.env` file.
*   Provides utility functions for getting port numbers (`get_port(service_name)`), checking port availability (`is_port_in_use()`, `find_available_port()`), and managing conflicts.
*   Ensures consistent port usage across the application.

**Key Configuration Files:**
*   **`.env`**: Contains environment variable overrides for port values and other settings. A template is usually provided as `config/env.sample`.
*   **`config/env.docker`**: Template for Docker-specific environment variables.
*   **[`src/config/ports.py`](src/config/ports.py)**: Defines default ports and utility functions.
*   **`docker-compose.yml`**: Defines port mappings for Docker containers.
*   **`$HOME/.graphrag/config.env`**: User-specific configuration for local non-Docker deployments, including database URIs, credentials, and service ports.

**Accessing Configuration in Code:**
*   **Python:** Use `from src.config import get_port` to retrieve port numbers.
    ```python
    from src.config import get_port
    api_port = get_port('api')
    ```
*   **Shell Scripts:** Source the `.env` file to use environment variables directly, or use a Python one-liner to call `get_port()`.
    ```bash
    # Source .env
    source .env
    echo "API Port: $GRAPHRAG_PORT_API"

    # Or via Python
    API_PORT=$(python -c "from src.config import get_port; print(get_port('api'))")
    ```
*   **Other Configuration Files (e.g., JSON):** Use environment variable substitution (e.g., `"${GRAPHRAG_PORT_MPC}"`).

**Updating Port References:**
*   The script [`scripts/update_port_references.py`](scripts/update_port_references.py) can help find hardcoded port references.
*   Manual updates involve replacing hardcoded ports with calls to `get_port()` in Python, or environment variables in scripts and other config files.

## Code Samples

This section provides illustrative code snippets found within the design documentation. For more detailed examples, refer to the `examples/` directory in the codebase and specific sections of [`specs/design/design.md`](specs/design/design.md).

**1. Adding Documents to ChromaDB (from `design.md`):**
```python
# Example of adding documents to ChromaDB
collection.add_documents(
    documents=[
        "Neural networks are a fundamental component of deep learning...",
        "The architecture of neural networks consists of layers..."
    ],
    embeddings=[
        [0.1, 0.2, 0.3, ...],  # 1024-dimensional embedding vector
        [0.2, 0.3, 0.4, ...]   # 1024-dimensional embedding vector
    ],
    metadatas=[
        {
            "document_id": "doc123",
            "title": "Deep Learning Fundamentals",
            "page": 42,
            "chunk_id": "chunk1",
            # ... other metadata ...
        },
        # ... more documents ...
    ],
    ids=["chunk1", "chunk2"]
)
```

**2. Document Chunker Configuration (from `design.md`):**
```python
from src.processing import DocumentChunker # Assuming path

chunker = DocumentChunker(
    chunk_size=512,
    chunk_overlap=50,
    respect_sections=True,
    split_on_headings=True
)
# chunks = chunker.chunk_text(document_text) # Example usage
```

**3. LLM Prompt Template for Concept Extraction (from `design.md`):**
```text
Extract the most important concepts from the following text and identify relationships between them.

For each concept, provide:
1. The concept name
2. A category (e.g., technique, theory, tool, person)
3. A confidence score (0-1)
4. A brief definition or explanation

Then, identify relationships between these concepts and any relevant concepts in the existing knowledge base.
Use specific relationship types such as:
- DEFINES_CONCEPT/EXPLAINS_TERM
- IS_A/TYPE_OF
# ... other relationship types ...

Existing concepts in the knowledge base:
{existing_concepts}

Text: {chunk_text}
```

**4. Neo4j Cypher Query for Index Creation (from `design.md` / `database_optimizations.md`):**
```cypher
CREATE INDEX book_title IF NOT EXISTS FOR (b:Book) ON (b.title);
CREATE INDEX concept_name IF NOT EXISTS FOR (c:Concept) ON (c.name);
CREATE INDEX section_title IF NOT EXISTS FOR (s:Section) ON (s.title);
```

**5. LangChain Agent Tool Usage (Conceptual, from `design.md`):**
```python
# from langchain.agents import initialize_agent, Tool
# from langchain_openai import ChatOpenAI # or other LLM
# from src.agents.langchain_tools import get_graphrag_tool_search # Example tool

# llm = ChatOpenAI(temperature=0, model_name="gpt-3.5-turbo")
# tools = [get_graphrag_tool_search()] # Simplified
# agent = initialize_agent(tools, llm, agent="zero-shot-react-description", verbose=True)
# agent.run("What are the main concepts related to hybrid search in GraphRAG?")
```
(Note: For full LangChain and OpenAI function calling examples, see Section 4 of `design.md`.)

## Networking

This section details the network architecture, focusing on port assignments and inter-service communication.

**Default Port Assignments:**

The system uses a set of default ports for its various services. These are defined in [`src/config/ports.py`](src/config/ports.py) and can be overridden by environment variables.

| Service         | Default Port | Env Variable Suffix | Description                        | Docker Mapping |
|-----------------|--------------|---------------------|------------------------------------|----------------|
| API             | 5001         | `API`               | Main GraphRAG API                  | `5001:5000`    |
| MPC             | 8765         | `MPC`               | Message Passing Communication server | `8765:8765`    |
| MCP             | 8767         | `MCP`               | Model Context Protocol server      | `8767:8767`    |
| Bug MCP         | 5005         | `BUG_MCP`           | Bug tracking MCP server            | N/A            |
| Neo4j Bolt      | 7687         | `NEO4J_BOLT`        | Neo4j Bolt protocol                | `7688:7687`    |
| Neo4j HTTP      | 7474         | `NEO4J_HTTP`        | Neo4j HTTP API (Browser)           | `7475:7474`    |
| Neo4j HTTPS     | 7473         | `NEO4J_HTTPS`       | Neo4j HTTPS API                    | N/A            |
| Prometheus      | 9090         | `PROMETHEUS`        | Prometheus metrics                 | N/A            |
| Grafana         | 3000         | `GRAFANA`           | Grafana dashboard                  | N/A            |
| Docker Neo4j Bolt | 7688       | `DOCKER_NEO4J_BOLT` | Docker mapping for Neo4j Bolt    | (Host side)    |

**Docker Port Mappings:**
When running within Docker (defined in `docker-compose.yml`), host ports are mapped to container ports. For example:
*   Neo4j Browser: Host `7475` -> Container `7474`
*   Neo4j Bolt: Host `7688` -> Container `7687`
*   API Server: Host `5001` -> Container `5000` (Note: API server inside Docker listens on 5000 by default)

**Service Communication:**
*   **Neo4j Database:** Accessed via Bolt protocol (e.g., `bolt://localhost:7687` locally, `bolt://0.0.0.0:7687` inside Docker, or `bolt://localhost:7688` from host to Docker).
*   **API Server:** HTTP (e.g., `http://localhost:5001`).
*   **MPC/MCP Servers:** WebSocket (e.g., `ws://localhost:8765` for MPC, `ws://localhost:8767` for MCP).

**Port Usage in Components:**
*   **`docker-entrypoint.sh`**: Starts services, respecting some environment variables for ports (e.g., `GRAPHRAG_API_PORT`) but with some hardcoded values or different internal defaults (e.g., API server listens on 5000 internally).
*   **`tools/test_setup.py`**: Uses the centralized port configuration and can dynamically adjust ports if conflicts are detected.
*   **Agent Tools (`src/agent-tools/utils.py`)**: Have their own default configurations for connecting to services, which should align with the centralized system.
*   **`scripts/check_ports.sh`**: Checks a predefined list of ports before starting Docker containers.

**Known Port Inconsistencies and Issues (to be addressed):**
Several documents (`port-mapping-issues.md`, `network-design.md`) list historical or current inconsistencies where different parts of the system might refer to or expect different ports for the same service (e.g., MPC port being 8765, 8766, or 8767 in different places). The goal of the centralized port configuration is to resolve these. Key areas of past inconsistency include:
*   MPC server port in `check_ports.sh`, agent tools, `docker-entrypoint.sh` output, and local `config.env`.
*   MCP server port in some documentation and hardcoding in `src/mpc/mcp_server.py`.
*   Bug MCP server port hardcoded in `bugMCP/bugMCP.py`.
The ongoing effort is to ensure all components correctly use the `src/config/ports.py` module or the derived environment variables.

## Deployment Guide

This section outlines how to deploy the GraphRAG system.

### Local Deployment (macOS without Docker)

**Prerequisites:**

*   macOS 10.15+
*   Python 3.8+
*   Neo4j 5.x
*   Minimum 4GB RAM, 10GB disk space

**Installation Steps:**

1.  **Clone Repository:**
    ```bash
    git clone https://github.com/yourusername/graphRAG.git
    cd graphRAG
    ```
2.  **Install Python Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
3.  **Install and Configure Neo4j:**
    *   Download Neo4j (e.g., version 5.18.1) and extract to `~/.graphrag/neo4j`.
    *   Modify `~/.graphrag/neo4j/conf/neo4j.conf` for listen addresses, memory settings (heap, pagecache), and enable authentication.
        *   Example memory settings: `dbms.memory.heap.initial_size=1024m`, `dbms.memory.heap.max_size=2048m`, `dbms.memory.pagecache.size=1024m`.
4.  **Configure GraphRAG:**
    *   Create `~/.graphrag/config.env` with settings for `NEO4J_HOME`, `NEO4J_URI`, `NEO4J_USERNAME`, `NEO4J_PASSWORD`, `CHROMA_PERSIST_DIRECTORY`, API/MPC ports, and log level.
5.  **Initialize Neo4j:**
    *   Start Neo4j: `~/.graphrag/neo4j/bin/neo4j start`
    *   Set initial password: `~/.graphrag/neo4j/bin/neo4j-admin dbms set-initial-password graphrag`
    *   Restart Neo4j.
6.  **Initialize GraphRAG Database:**
    ```bash
    python scripts/initialize_database.py
    ```

**Running GraphRAG Services:**

*   **Using Service Script (`scripts/service_management/graphrag-service.sh`):**
    *   Provides commands to `start`, `stop`, `restart`, and check `status` of all services or individual services (neo4j, api, mpc).
    ```bash
    chmod +x scripts/service_management/graphrag-service.sh
    scripts/service_management/graphrag-service.sh start
    ```
*   **Service Monitoring (`bin/graphrag-monitor.py`):**
    *   Monitors services and restarts them if they crash.
    *   Can be run in the background using `nohup`.

**Verifying Installation:**

*   **Neo4j:** Check Neo4j Browser (e.g., `curl -u neo4j:graphrag http://localhost:7474/browser/`).
*   **ChromaDB:** Run `python scripts/verify_chromadb.py`.
*   **API Server:** Check health endpoint (e.g., `curl http://localhost:5001/health`).
*   **MPC Server:** Test with `python examples/mpc_client_example.py --tool ping`.

**Adding Documents:**

*   Single document: `python scripts/document_processing/add_document_core.py --file /path/to/document.pdf`
*   Folder of documents: `python scripts/document_processing/add_pdf_documents.py --folder /path/to/documents --recursive`

**Troubleshooting:**

*   Check logs in `~/.graphrag/logs/`.
*   For Neo4j auth issues, reset password using `neo4j-admin`.
*   For ChromaDB issues, check version, verify, or clear data (`rm -rf ~/.graphrag/data/chromadb/*`).

**Uninstalling:**

*   Stop services using `graphrag-service.sh stop`.
*   Remove data: `rm -rf ~/.graphrag`.

**(Note: For Docker deployment and Production Deployment Topology, refer to the main `design.md` document.)**

## Database Design and Optimisation

The GraphRAG system utilizes two primary databases: Neo4j for the knowledge graph and ChromaDB for vector embeddings. Optimizations have been implemented for both to handle large data volumes effectively.

### Neo4j (Graph Database)

**Schema:**
(Refer to the Neo4j Schema section in `design.md` for a detailed ERD and node/relationship definitions. Key entities include Book, Chapter, Section, and Concept.)

**Optimizations:**

*   **Memory Configuration:**
    *   Optimized Neo4j memory settings for improved performance with large graphs:
        ```
        dbms.memory.heap.initial_size=1024m
        dbms.memory.heap.max_size=2048m
        dbms.memory.pagecache.size=1024m
        dbms.memory.transaction.total.max=512m
        db.memory.transaction.max=64m
        ```
*   **Additional Indexes:**
    *   Indexes are created on various properties of `Book`, `Chapter`, `Section`, and `Concept` nodes to enhance query performance. Examples include:
        ```cypher
        CREATE INDEX book_title IF NOT EXISTS FOR (b:Book) ON (b.title);
        CREATE INDEX concept_name IF NOT EXISTS FOR (c:Concept) ON (c.name);
        // ... and other indexes for categories, IDs, etc.
        ```

### ChromaDB (Vector Database)

**Collection Structure:**
(Refer to the ChromaDB Collection Structure section in `design.md` for details on how document chunks, embeddings, and metadata are stored.)

**Optimizations:**

*   **HNSW Index Configuration:**
    *   Optimized HNSW (Hierarchical Navigable Small World) index parameters for large datasets to balance search accuracy and performance:
        ```python
        collection_metadata = {
            "hnsw:space": "cosine",
            "hnsw:construction_ef": 128,
            "hnsw:search_ef": 128,
            "hnsw:M": 16,
            "hnsw:num_threads": 4
        }
        ```
*   **Smart Chunking Strategy:**
    *   Employs a semantic chunking strategy that respects paragraph and sentence boundaries, with configurable overlap to maintain context. This improves the quality of vector search results.
    *   Example: `smart_chunk_text(text, chunk_size=1000, overlap=200, semantic_boundaries=True)`
*   **Batch Processing:**
    *   Documents are added to ChromaDB in batches (e.g., batch size of 100) to optimize performance and memory usage.
*   **Metadata Optimization:**
    *   Metadata is processed to be more search-friendly for ChromaDB (e.g., converting lists to strings, flattening dictionaries, adding lowercase fields).

### General Performance Considerations

*   **Memory Usage:** Optimized settings aim for systems with at least 4GB RAM. Adjustments to Neo4j heap/pagecache and batch sizes may be needed for systems with less memory.
*   **Disk Space:** The chunking strategy might increase the number of chunks, requiring sufficient disk space.
*   **Processing Time:** Optimizations prioritize search quality and accuracy, which might mean slower initial ingestion but improved query performance.

### Monitoring and Tuning

*   Regular monitoring of system performance is recommended.
*   Neo4j Browser can be used to monitor query performance.
*   Adjustments to memory settings, batch sizes, and HNSW parameters can be made based on observed performance and resource constraints.

### Optimized Scripts

*   [`scripts/optimize_existing_data.py`](scripts/optimize_existing_data.py): Optimizes existing data in the vector database.

## MCP (Mode Control Protocol) Design

The MCP (Model Context Protocol) server allows AI assistants (like Claude) to interact with the GraphRAG system, enabling them to use GraphRAG's capabilities as tools.

**Purpose:**
*   Provides a standardized way for AI assistants to:
    *   Search the GraphRAG knowledge base.
    *   Add documents to the system.
    *   Explore concepts and their relationships.
    *   Find passages about specific concepts.

**Prerequisites for Setup:**
*   GraphRAG installed and configured.
*   Neo4j database running and accessible.
*   ChromaDB initialized.
*   Python 3.11+ and required packages (websockets, neo4j, chromadb).

**Server Configuration:**
*   The MCP server uses environment variables for database connections:
    *   `NEO4J_URI`
    *   `NEO4J_USERNAME`
    *   `NEO4J_PASSWORD`
    *   `CHROMA_PERSIST_DIRECTORY`
*   **AI Assistant Configuration (`mcp_settings.json`):**
    *   An `mcp_settings.json` file is required in the AI assistant's configuration directory to define how it connects to and launches the GraphRAG MCP server.
    *   This file specifies the command to run the MCP server, arguments (host, port), and necessary environment variables (PYTHONPATH, database credentials).
    *   Example `mcp_settings.json` content:
      ```json
      {
        "mcpServers": {
          "GraphRAG": {
            "command": "/path/to/graphRAG/tools/graphrag-mcp",
            "args": ["--host", "0.0.0.0", "--port", "8767"],
            "env": {
              "PYTHONPATH": "/path/to/graphRAG",
              "NEO4J_URI": "bolt://localhost:${GRAPHRAG_PORT_DOCKER_NEO4J_BOLT}",
              "NEO4J_USERNAME": "neo4j",
              "NEO4J_PASSWORD": "graphrag",
              "CHROMA_PERSIST_DIRECTORY": "/path/to/graphRAG/data/chromadb"
            }
          }
        }
      }
      ```
    *   The `tools/generate-mcp-settings.py` script can be used to generate this file.

**Starting the MCP Server:**
*   **Wrapper Script:** `./tools/graphrag-mcp` (sets PYTHONPATH, installs dependencies, verifies Neo4j, starts server on port 8767 by default).
*   **Docker:** Automatically started with the container.
*   **Manual:** `export PYTHONPATH=/path/to/graphRAG; python -m src.mpc.mcp_server --host 0.0.0.0 --port ${GRAPHRAG_PORT_MCP}`

**Testing:**
*   Use `python tests/test_mcp_server.py` for automated tests.
*   Use `python scripts/mcp_client_example.py` for interactive testing.

**Available Tools (via MCP):**
*   `ping`: Connection test.
*   `search`: Hybrid search.
*   `concept`: Get concept information.
*   `documents`: Get documents for a concept.
*   `books-by-concept`: Find books mentioning a concept.
*   `related-concepts`: Find related concepts.
*   `passages-about-concept`: Find passages about a concept.
*   `add-document`: Add a single document.
*   `add-folder`: Add a folder of documents.
*   `job-status`: Get job status.
*   `list-jobs`: List all jobs.
*   `cancel-job`: Cancel a job.

**Advanced Configuration:**
*   **Port Changes:** Update in `tools/graphrag-mcp`, `mcp_settings.json`, and `docker-compose.yml` (if used).
*   **Production:** Use a process manager (Supervisor, systemd), set up logging, authentication, and consider a reverse proxy for SSL.

**Logging and Debugging:**
*   Logs to console by default. Redirect output to a file for persistent logs.
*   Use `--log-level DEBUG` for verbose logging.
*   Troubleshoot connection issues (to MCP server, Neo4j, ChromaDB), tool errors, and Claude integration problems by checking configurations, server status, and logs.

## Development Tools

The GraphRAG project includes several tools to aid in development, testing, and environment management.

*   **UV Wrapper Script (`tools/uvrun.sh`):**
    *   Ensures Python commands (install, run, test, lint, format) are executed using UV, the preferred package manager and runtime.
    *   Provides a command to activate the virtual environment (`./tools/uvrun.sh shell`).

*   **Bug Tracking Client (`tools/bug_client.py`):**
    *   A robust interface for the bug tracking system.
    *   Supports adding bugs (including critical ones), listing bugs, and syncing pending bugs.
    *   Includes error handling, retries, and fallback mechanisms.

*   **Test Environment Setup (`tools/test_setup.py`):**
    *   Manages the test environment by starting/stopping required services.
    *   Checks port availability and provides a clean shutdown mechanism.
    *   Commands: `--start`, `--stop`, `--status`.

*   **Regression Test Runner (`tools/run_regression_tests.sh`):**
    *   Executes regression tests with proper environment setup.

*   **Port Configuration (`src/config/ports.py`):**
    *   Centralized module for all port configurations used by GraphRAG services.
    *   Includes utilities for checking port availability and resolving conflicts.
    *   Default ports can be overridden using environment variables (e.g., `GRAPHRAG_PORT_MCP`).
    *   Key Default Ports:
        *   API: 5001
        *   MPC Server: 8765
        *   MCP Server: 8767
        *   Bug MCP Server: 5005
        *   Neo4j Bolt: 7687
        *   Neo4j HTTP: 7474

**Development Best Practices:**

1.  **Use UV:** Always use the `tools/uvrun.sh` wrapper for Python commands to ensure consistency.
2.  **Centralized Ports:** Import port settings from `src.config.ports` instead of hardcoding.
3.  **Bug Client:** Report bugs using `tools/bug_client.py`.
4.  **Test Setup:** Use `tools/test_setup.py` before running tests.
5.  **Regression Runner:** Use `tools/run_regression_tests.sh` for comprehensive testing.